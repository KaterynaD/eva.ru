{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19d607d6-38f8-44a4-85b2-9edfd1003d77",
   "metadata": {},
   "source": [
    "### Number of specific Part of Speech (POS) in Russian text, punctuation, length of words\n",
    "* Basic text attributes were calculated during download to be used in quick ongoing analysis\n",
    "* Even more complex coefficients can be calculated but number of sentences is required. Punctuation is frequently omitted in forum posts and everything which depend on it is not reliable. A special model to predict punctuation could be help\n",
    "https://linguistics.stackexchange.com/questions/3167/are-there-sentence-boundary-disambiguation-algorithms-which-can-handle-punctuati\n",
    "\n",
    "POS was analyzed using treetaggerwrapper which is just a wrapper for a treetagger standalone program. It's slow and ineffective. (Was done in a background mode based on a code from this notebook.) I should have used pymorphy2 library instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9155e1df-6df9-4f88-a7c5-d6f58e0af1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c747d40-be3f-4d3d-ae19-ddee1c3e8ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kate/anaconda3/lib/python3.8/site-packages/treetaggerwrapper.py:739: FutureWarning: Possible nested set at position 8\n",
      "  punct2find_re = re.compile(\"([^ ])([[\" + ALONEMARKS + \"])\",\n",
      "/home/kate/anaconda3/lib/python3.8/site-packages/treetaggerwrapper.py:2043: FutureWarning: Possible nested set at position 152\n",
      "  DnsHostMatch_re = re.compile(\"(\" + DnsHost_expression + \")\",\n",
      "/home/kate/anaconda3/lib/python3.8/site-packages/treetaggerwrapper.py:2067: FutureWarning: Possible nested set at position 409\n",
      "  UrlMatch_re = re.compile(UrlMatch_expression, re.VERBOSE | re.IGNORECASE)\n",
      "/home/kate/anaconda3/lib/python3.8/site-packages/treetaggerwrapper.py:2079: FutureWarning: Possible nested set at position 192\n",
      "  EmailMatch_re = re.compile(EmailMatch_expression, re.VERBOSE | re.IGNORECASE)\n"
     ]
    }
   ],
   "source": [
    "#!pip install treetaggerwrapper\n",
    "import treetaggerwrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cd417d3-834a-4f31-b33d-6c6f4b70fad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "def replace_consequent_symbols(text,whatever):\n",
    "    for k, g in groupby(text):\n",
    "        size = len(list(g))\n",
    "        if k in whatever:\n",
    "            text=text.replace(k * size,k)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dad7656-417d-4027-831b-f3b92fbe80c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of correspondent lemmas based on value in pos\n",
    "def num_pos(pos_to_count,pos_list,lemmas_list):\n",
    "    index_list=[idx for idx, val in enumerate(pos_list) if val.startswith(pos_to_count)]\n",
    "    num_pos=len(index_list)\n",
    "    num_unique_pos=len(set([lemmas_list[i] for i in index_list]))\n",
    "    return num_pos,num_unique_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc524d33-efd7-44a6-bf6e-fd7f34713f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of symbols from whatever in text\n",
    "def count_whatever(text,whatever):\n",
    "    cnt=0\n",
    "    for v in whatever:\n",
    "        cnt = cnt + text.count(v)\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "041842c9-e03c-48bb-9a3a-93dd7f35bd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start_Message_Id=int(sys.argv[1:][2])\n",
    "#End_Message_Id=int(sys.argv[1:][3])\n",
    "#Part=int(sys.argv[1:][4])\n",
    "Start_Message_Id=0\n",
    "End_Message_Id=1000000\n",
    "Part=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92298b86-7aa4-468f-83e7-83529b2c5688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from message number: 0\n",
      "Ending with message number: 1000000\n",
      "Result will be saved in 10 part\n"
     ]
    }
   ],
   "source": [
    "print('Starting from message number: %s'%Start_Message_Id)\n",
    "print('Ending with message number: %s'%End_Message_Id)\n",
    "print('Result will be saved in %s part'%Part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f525b32-4e24-40f1-8848-7a9396701c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data='/home/kate/Projects/eva/Data/Main'\n",
    "\n",
    "Message_filename='Messages.csv'#'Messages.csv'\n",
    "Message_full_filename=os.path.join(Data, Message_filename)\n",
    "text_column='message'\n",
    "\n",
    "ta_extended_extension_filename='Extended/TextExtendedAttributes_ext_%s.csv'%Part\n",
    "ta_extended_extension_full_filename=os.path.join(Data, ta_extended_extension_filename)\n",
    "\n",
    "long_word_syllabels=3\n",
    "rus_vowels = 'аэыуояеёюи'\n",
    "replace_consequent='!?.,)()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c04f5e1-d189-405c-9ef2-a38d4cb04708",
   "metadata": {},
   "outputs": [],
   "source": [
    "Messages = pd.read_csv(Message_full_filename, error_bad_lines=False, index_col=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b3aabfd-3cc6-457c-939b-3f742f83333a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SubsetToAnalyze=Messages[Start_Message_Id:End_Message_Id+1:1].copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43ea0b36-f29b-4142-80ea-8388695cb3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "del Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67cdc9db-bebf-4f2c-98d9-432b0a1cd4c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(SubsetToAnalyze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "782449ff-5334-4ecc-b3c5-47e63022d822",
   "metadata": {},
   "outputs": [],
   "source": [
    "SubsetToAnalyze[text_column]=SubsetToAnalyze[text_column].fillna(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c10e00a-c013-4e93-a3d8-1c2eea1683c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = treetaggerwrapper.TreeTagger(TAGLANG='ru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b18f81b8-2c9d-4124-8b41-566809e0dfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_all_metrics(m):\n",
    "    metrics=list()\n",
    "    m=m+'.'\n",
    "    \n",
    "    #if there are !!!!!!!!!! or ??????? or ?!!! or !????\n",
    "    flg_excessive_exclamations=1 if '!!' in m else 0\n",
    "    metrics.append(flg_excessive_exclamations)\n",
    "    flg_excessive_questions=1 if '??' in m else 0\n",
    "    metrics.append(flg_excessive_questions)\n",
    "    flg_excessive_other=1 if (('?!' in m) | ('!?' in m)) else 0\n",
    "    metrics.append(flg_excessive_other)\n",
    "\n",
    "    #replacing excessive !? to correctly count sentences\n",
    "    m=replace_consequent_symbols(m,replace_consequent)\n",
    "    m = m.replace('?!','?').replace('!?','!')\n",
    "\n",
    "    \n",
    "    #defining POS (part-of-speach) and corresponding lammas\n",
    "    ctags = tagger.tag_text(m)\n",
    "    #currupted text may result in missing/t\n",
    "    ctags=[i if len(i.split('\\t', 2))==3 else '-\\t-\\t\\-t' for i in ctags]\n",
    "    pos=[i.split('\\t', 2)[1] for i in ctags]\n",
    "    lemma=[i.split('\\t', 3)[2] for i in ctags]\n",
    "\n",
    "    #number of sentences is incorrect in many cases because punctuation signs are missing or there is no space after \n",
    "    #need a finetuned model to predict end of sentence for russian. \n",
    "    #https://linguistics.stackexchange.com/questions/3167/are-there-sentence-boundary-disambiguation-algorithms-which-can-handle-punctuati\n",
    "    #num_sentences=pos.count('SENT')\n",
    "    #if num_sentences==0:\n",
    "    #    num_sentences=1\n",
    "    #metrics.append(num_sentences)\n",
    "\n",
    "    #number of nouns(Nc), verbs(Vm) and adjective(Af):\n",
    "    (num_Adj, num_unique_Adj)=num_pos('Af',pos,lemma)\n",
    "    if num_Adj==0:\n",
    "        num_Adj=1\n",
    "    metrics.append(num_Adj)\n",
    "    metrics.append(num_unique_Adj)\n",
    "    (num_Nouns, num_unique_Nouns)=num_pos('Nc',pos,lemma)\n",
    "    if num_Nouns==0:\n",
    "        num_Nouns=1    \n",
    "    metrics.append(num_Nouns)\n",
    "    metrics.append(num_unique_Nouns)\n",
    "    (num_Verb, num_unique_Verb)=num_pos('Vm',pos,lemma)\n",
    "    if num_Verb==0:\n",
    "        num_Verb=1        \n",
    "    metrics.append(num_Verb)\n",
    "    metrics.append(num_unique_Verb)\n",
    "    num_tokens=num_Adj + num_Nouns + num_Verb\n",
    "    metrics.append(num_tokens)\n",
    "    num_unique_tokens=num_unique_Adj + num_unique_Nouns + num_unique_Verb\n",
    "    metrics.append(num_unique_tokens)\n",
    "\n",
    "    #number of syllables and long words based on vowels\n",
    "    vowels = [count_whatever(i,rus_vowels) for i in lemma]\n",
    "    num_syllables=sum(vowels)\n",
    "    metrics.append(num_syllables)\n",
    "    index_list=[idx for idx, val in enumerate(vowels) if val>long_word_syllabels]\n",
    "    num_long_words = len([lemma[i] for i in index_list])\n",
    "    metrics.append(num_long_words)\n",
    "    num_unique_long_words = len(set([lemma[i] for i in index_list]))\n",
    "    metrics.append(num_unique_long_words)\n",
    "\n",
    "    #number of !,? and ,\n",
    "    #Error in num_commas!!!!\n",
    "    num_commas=count_whatever(m,'!')\n",
    "    metrics.append(num_commas)\n",
    "    num_exclamations=count_whatever(m,'!')\n",
    "    metrics.append(num_exclamations)\n",
    "    num_questions=count_whatever(m,'?')\n",
    "    metrics.append(num_questions)\n",
    "\n",
    "    #no need to calculate, already in the dataset but needed for the further calculation\n",
    "    num_words=len(m.split(' '))\n",
    "    metrics.append(num_words)\n",
    "    #avg_sent_words = num_words/num_sentences\n",
    "    #metrics.append(avg_sent_words)\n",
    "\n",
    "    #ratio calculations based on the above\n",
    "    #ASL = num_words/num_sentences\n",
    "    #metrics.append(ASL)\n",
    "    ASW = num_syllables/num_words\n",
    "    metrics.append(ASW)\n",
    "    PLW = num_long_words/num_words\n",
    "    metrics.append(PLW)\n",
    "    TTR = num_unique_tokens/num_tokens\n",
    "    metrics.append(TTR)\n",
    "    TTR_A = num_unique_Adj/num_Adj\n",
    "    metrics.append(TTR_A)\n",
    "    TTR_N = num_unique_Nouns/num_Nouns\n",
    "    metrics.append(TTR_N)\n",
    "    TTR_V = num_unique_Verb/num_Verb\n",
    "    metrics.append(TTR_V)\n",
    "    if TTR_V!=0:\n",
    "        NAV = (TTR_A +TTR_N)/TTR_V\n",
    "    else:\n",
    "        NAV = 0\n",
    "    metrics.append(NAV)\n",
    "    if num_unique_Verb!=0:\n",
    "        UNAV = (num_unique_Adj +num_unique_Nouns)/num_unique_Verb\n",
    "    else:\n",
    "        UNAV = 0\n",
    "    metrics.append(UNAV)\n",
    "    fraction_of_commas=num_commas/num_words\n",
    "    metrics.append(fraction_of_commas)\n",
    "    fraction_of_exclamations=num_exclamations/num_words\n",
    "    metrics.append(fraction_of_exclamations)\n",
    "    fraction_of_questions=num_questions/num_words\n",
    "    metrics.append(fraction_of_questions)\n",
    "    fraction_of_Adj=num_Adj/num_words\n",
    "    metrics.append(fraction_of_Adj)\n",
    "    fraction_of_Nouns=num_Nouns/num_words\n",
    "    metrics.append(fraction_of_Nouns)\n",
    "    fraction_of_Verbs= num_Verbs/num_words\n",
    "    metrics.append(fraction_of_Verbs)\n",
    "    \n",
    "        \n",
    "    #Almost all coefficients are based on ASL which is not reliable because num_sentences issue See above\n",
    "    #Also the same number of tokens should be taken into account in analysis which is not strightforward in forum posts\n",
    "    #It does not make sense to use them in further analysis for now\n",
    "    #Readability_Z1 = (0.62 * ASL) + (0.123 * PLW) + 0.051\n",
    "    #metrics.append(Readability_Z1)\n",
    "    #F4 =0.83*UNAV-6.73*TTR+0.24*ASL+3.36*ASW-2.41\n",
    "    #metrics.append(F4)\n",
    "    #F5 =0.81*UNAV-5.47*TTR_A+0.24*ASL+3.28*ASW-0.6*NAV-1.79\n",
    "    #metrics.append(F5)\n",
    "    #Q = - 0.124*ASL + 0.018*ASW - 0.007*UNAV + 0.007*NAV - 0.003*math.pow(ASL,2) + 0.184*ASL*ASW + 0.097*ASL*UNAV - 0.158*ASL*NAV + 0.09*math.pow(ASW,2) + 0.091*ASW*UNAV + 0.023*ASW*NAV - 0.157*math.pow(UNAV,2) - 0.079*UNAV*NAV + 0.058*math.pow(NAV,2)\n",
    "    #metrics.append(Q)\n",
    "    #Z2 = 0.5+ASL+8.4+ASW-15.59\n",
    "    #metrics.append(Z2)\n",
    "    df = pd.DataFrame([metrics],columns=[\n",
    "'flg_excessive_exclamations',\n",
    "'flg_excessive_questions',\n",
    "'flg_excessive_other',\n",
    "#'num_sentences',\n",
    "'num_Adj',\n",
    "'num_unique_Adj',\n",
    "'num_Nouns',\n",
    "'num_unique_Nouns',\n",
    "'num_Verb',\n",
    "'num_unique_Verb',\n",
    "'num_tokens',\n",
    "'num_unique_tokens',\n",
    "'num_syllables',\n",
    "'num_long_words',\n",
    "'num_unique_long_words',\n",
    "'num_commas',\n",
    "'num_exclamations',\n",
    "'num_questions',\n",
    "'num_words',\n",
    "#'avg_sent_words',\n",
    "#'ASL',\n",
    "'ASW',\n",
    "'PLW',\n",
    "'TTR',\n",
    "'TTR_A',\n",
    "'TTR_N',\n",
    "'TTR_V',\n",
    "'NAV',\n",
    "'UNAV',\n",
    "'fraction_of_commas',\n",
    "'fraction_of_exclamations',\n",
    "'fraction_of_questions',\n",
    "'fraction_of_Adj',\n",
    "'fraction_of_Nouns',\n",
    "'fraction_of_Verbs'\n",
    "#'Readability_Z1',\n",
    "#'F4',\n",
    "#'F5',\n",
    "#'Q',\n",
    "#'Z2'\n",
    "    ])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d821f3ca-07a1-46a3-ab3d-2b1496aaf1e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-11-25 08:13:53.189519\n",
      "2021-11-25 08:13:54.320230\n",
      "\n",
      "Processing  complete\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())\n",
    "TextAttr_df=pd.DataFrame()\n",
    "for index, row in SubsetToAnalyze.iterrows():\n",
    "    #print(len(row[text_column]))\n",
    "    df=calculate_all_metrics(row[text_column])\n",
    "    df['Message_Id']=row['Message_Id']\n",
    "    TextAttr_df=TextAttr_df.append(df)\n",
    "    #print('.', end = ' '),\n",
    "print(datetime.datetime.now()) \n",
    "TextAttr_df.to_csv(ta_extended_extension_full_filename, header=True, index=False) \n",
    "print()\n",
    "print('Processing  complete')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebf6125-2a91-43ad-902f-5a58d1cc9744",
   "metadata": {},
   "source": [
    "#Error in num_commas!!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
