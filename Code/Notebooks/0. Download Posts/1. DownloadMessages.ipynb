{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9faacb2c-55b3-4897-8b7d-b20cb064bed1",
   "metadata": {},
   "source": [
    "The notebook downloads eva.ru forum topics using links extracted via DownloadTopicsLinks.py and saved in Chapters-XYZ.csv (XYZ is a code of a specific forum) Then it extracts individual posts and their attributes, clean up posts from links, images, emojis and save data in Messages.csv preserving the original post content. This is a prototype of DownloadMessages.py Python code which was run for days in a background mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eb4b075-f581-4a04-9ee0-f168fd9ad1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from emoji import UNICODE_EMOJI\n",
    "import re\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09bcc233-b188-4979-8365-07b17cbe75de",
   "metadata": {},
   "outputs": [],
   "source": [
    "Chapter_Id_To_Load=77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19220c95-ae1d-49de-a117-385287ae497c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnt_emoji(s):\n",
    "    count = 0\n",
    "    for emoji in UNICODE_EMOJI['en']:\n",
    "        count += s.count(emoji)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d22d31d1-9b3a-4f1e-922c-be34ecf6404e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_urls (vTEXT):\n",
    "    vTEXT = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%|\\-)*\\b', '', vTEXT, flags=re.MULTILINE)\n",
    "    return(vTEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdf29398-cd2f-495e-97e9-3b7cb7d1e4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Base_URL='https://eva.ru'\n",
    "Data='/home/kate/Projects/eva/Data'\n",
    "Chapters_filename='Chapters-%s.csv'%Chapter_Id_To_Load\n",
    "Chapters_full_filename=os.path.join(Data, Chapters_filename)\n",
    "Topics_links_filename='Topics.csv'\n",
    "Topics_links_full_filename=os.path.join(Data, Topics_links_filename)\n",
    "Messages_filename='Messages.csv'\n",
    "Messages_full_filename=os.path.join(Data, Messages_filename)\n",
    "max_count_messages_in_file=100000\n",
    "attempts=10\n",
    "delay=60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ff4b9a9-0b67-447f-aab4-8d53cae466c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetMessages(Topic_Url):\n",
    "    html_page = urllib.request.urlopen(Base_URL+Topic_Url)\n",
    "    soup = BeautifulSoup(html_page)\n",
    "    Message_Ids = list()\n",
    "    Parent_Ids = list()\n",
    "    Date_Time = list()\n",
    "    Author_Ids = list()\n",
    "    Authors = list()\n",
    "    Original_Messages = list()\n",
    "    Messages = list()\n",
    "    Message_chars = list()\n",
    "    Message_words = list()\n",
    "    Emojis = list()\n",
    "    Images = list()\n",
    "    Links = list()\n",
    "    Original_Paragraphs = list()\n",
    "    Actual_Paragraphs = list()\n",
    "    Avg_Paragraph_chars = list()\n",
    "    Avg_Paragraph_words = list()\n",
    "    items = soup.findAll('li', {'class': 'item'})\n",
    "    for item in items: \n",
    "        for message_id in item.findAll('div', {'class': 'id'}): \n",
    "            Message_Ids.append(message_id.text.strip().rstrip('\\n').lstrip('\\n')[1:])\n",
    "            p_i=0\n",
    "            for parent_id in item.findAll('div', {'class': 'message-id'}):\n",
    "                p_i=parent_id.text.strip().rstrip('\\n').lstrip('\\n')[10:]         \n",
    "            Parent_Ids.append(p_i) \n",
    "            for date_time in item.findAll('div', {'class': 'date-time'}):\n",
    "                Date_Time.append(date_time.text.strip().rstrip('\\n').lstrip('\\n')[1:])         \n",
    "            for author in item.findAll('div', {'class': 'author'}): \n",
    "                Authors.append(remove_urls(author.text.strip().rstrip('\\n').lstrip('\\n')))\n",
    "                a_i=0\n",
    "                for author_id in author.findAll('a', href=True):\n",
    "                    a_i=author_id['href'][1:]\n",
    "                Author_Ids.append(a_i)\n",
    "            for message in item.findAll('div', {'class': 'body'}):\n",
    "                Original_Messages.append(message)\n",
    "                message_content=remove_urls(message.text.strip().rstrip('\\n').lstrip('\\n'))\n",
    "                Messages.append(message_content)\n",
    "                Message_chars.append(len(message_content))\n",
    "                Message_words.append(len(message_content.split()))\n",
    "                Emojis1 = 0\n",
    "                Img = 0\n",
    "                for img in message.findAll('img', src=True):\n",
    "                    if '/design/eva/images/forum/' in img['src']:\n",
    "                        Emojis1 = Emojis1 + 1\n",
    "                    else:\n",
    "                        Img = Img + 1 \n",
    "                Images.append(Img)\n",
    "                Emojis2=cnt_emoji(message_content)\n",
    "                Emojis.append(Emojis1 + Emojis2)\n",
    "                Links.append(len(message.findAll('a')))\n",
    "                Original_Paragraphs.append(len(message.findAll('br')))\n",
    "                ap=message.get_text(strip=True, separator='<br/>').count('<br/>')\n",
    "                Actual_Paragraphs.append(ap)\n",
    "                if ap>1:\n",
    "                    pl = 0\n",
    "                    pw = 0\n",
    "                    for p in message.get_text(strip=True, separator='<br/>').split('<br/>'):\n",
    "                        pl = pl + len(p)\n",
    "                        pw = pw + len(p.split())\n",
    "                    Avg_Paragraph_chars.append(pl/ap)\n",
    "                    Avg_Paragraph_words.append(pw/ap)\n",
    "                else:\n",
    "                    Avg_Paragraph_chars.append(0)\n",
    "                    Avg_Paragraph_words.append(0)\n",
    "    return list(zip(Message_Ids, Parent_Ids, Date_Time, Author_Ids, Authors, Original_Messages, Messages, Message_chars,Message_words, Emojis, Images, Links, Original_Paragraphs,Actual_Paragraphs,  Avg_Paragraph_chars, Avg_Paragraph_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b940a6d-bfed-41e7-b87f-3e5019aa777a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chapter</th>\n",
       "      <th>Folder</th>\n",
       "      <th>Chapter_Id</th>\n",
       "      <th>StartFromPage</th>\n",
       "      <th>StopOnPage</th>\n",
       "      <th>ProcessedPage</th>\n",
       "      <th>cnt_Topics</th>\n",
       "      <th>min_Topic_Id</th>\n",
       "      <th>max_Topic_Id</th>\n",
       "      <th>StartFromTopic</th>\n",
       "      <th>StopOnTopic</th>\n",
       "      <th>LastProcessedTopic</th>\n",
       "      <th>cnt_ProcessedTopics</th>\n",
       "      <th>Messages_File</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Все остальное</td>\n",
       "      <td>beauty</td>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "      <td>100000</td>\n",
       "      <td>2704</td>\n",
       "      <td>54075</td>\n",
       "      <td>1544229</td>\n",
       "      <td>3620944</td>\n",
       "      <td>1544229</td>\n",
       "      <td>1544229</td>\n",
       "      <td>1544229</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Chapter  Folder  Chapter_Id  StartFromPage  StopOnPage  \\\n",
       "0  Все остальное  beauty          77              1      100000   \n",
       "\n",
       "   ProcessedPage  cnt_Topics  min_Topic_Id  max_Topic_Id  StartFromTopic  \\\n",
       "0           2704       54075       1544229       3620944         1544229   \n",
       "\n",
       "   StopOnTopic  LastProcessedTopic  cnt_ProcessedTopics  Messages_File  \n",
       "0      1544229             1544229                    0              1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Chapters = pd.read_csv(Chapters_full_filename, error_bad_lines=False, index_col=False) \n",
    "Chapters[Chapters['Chapter_Id']==Chapter_Id_To_Load]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9428c33-aad0-46f8-b512-fb313302cbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Topics = pd.read_csv(Topics_links_full_filename, error_bad_lines=False, index_col=False) \n",
    "Topics.sort_values(by=['Chapter_Id','Topic_Id'], ascending=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbf3054-6bc3-4411-980a-e481c1470c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c_index, c_row in Chapters[Chapters['Chapter_Id']==Chapter_Id_To_Load].iterrows():\n",
    "    File_Num=c_row['Messages_File']\n",
    "    Messages_full_filename=os.path.join(Data, '%s-%s.csv'%(c_row['Chapter_Id'],File_Num))\n",
    "    Messages=pd.DataFrame(columns=['Message_Id','Parent_Id','date_time','Author_Id','author','original_message','message','message_characters','message_words','emojis','images','links','original_paragraphs','actual_paragraphs',' avg_paragraph_characters', 'avg_paragraph_words','Topic_Id','Topic','Chapter_Id','Chapter'])\n",
    "    #Messages = pd.read_csv(Messages_full_filename, error_bad_lines=False, index_col=False)\n",
    "    print('Processing Chapter: %s(%s)'%(c_row['Chapter'],c_row['Chapter_Id']))\n",
    "    cnt_topics=c_row['cnt_ProcessedTopics']\n",
    "    for t_index, t_row in Topics[((Topics['Chapter_Id']==c_row['Chapter_Id']) & (Topics['Topic_Id']>=c_row['StartFromTopic']))].iterrows():\n",
    "        #print('Processing Topic: %s(%s)'%(t_row['Topic'],t_row['Topic_Id']))\n",
    "        print('.', end = ' '),\n",
    "        cnt=0\n",
    "        while cnt<attempts:\n",
    "            try:\n",
    "                t=GetMessages(t_row['Link'])\n",
    "                if len(t)==0:\n",
    "                    print()\n",
    "                    print('No messages found in topic %s'%t_row['Link'])\n",
    "                cnt=attempts\n",
    "            except Exception as e:\n",
    "                print('Something went wrong...'+ str(e))\n",
    "                cnt=cnt+1\n",
    "                print('Waiting...')\n",
    "                time.sleep(delay)\n",
    "                print('Attempt to read topic %s # %s'%(t_row['Topic_Id'],cnt+1))\n",
    "        if cnt>attempts:\n",
    "            print('Too many false attempts. Stop execution...')\n",
    "            break\n",
    "        t_pd = pd.DataFrame(t, columns=['Message_Id','Parent_Id','date_time','Author_Id','author','original_message','message','message_characters','message_words','emojis','images','links','original_paragraphs','actual_paragraphs',' avg_paragraph_characters', 'avg_paragraph_words'])\n",
    "        t_pd['Topic_Id']=t_row['Topic_Id']\n",
    "        t_pd['Topic']=t_row['Topic']\n",
    "        t_pd['Chapter_Id']=c_row['Chapter_Id']\n",
    "        t_pd['Chapter']=c_row['Chapter']\n",
    "        Messages=Messages.append(t_pd)\n",
    "        Messages.to_csv(Messages_full_filename, header=True, index=False)\n",
    "        if len(Messages)>max_count_messages_in_file:\n",
    "            File_Num=File_Num+1\n",
    "            Chapters.at[c_index,'Messages_File']=File_Num\n",
    "            Messages_full_filename=os.path.join(Data, '%s-%s.csv'%(c_row['Chapter_Id'],File_Num))\n",
    "            Messages=pd.DataFrame(columns=['Message_Id','Parent_Id','date_time','Author_Id','author','original_message','message','message_characters','message_words','emojis','images','links','original_paragraphs','actual_paragraphs',' avg_paragraph_characters', 'avg_paragraph_words','Topic_Id','Topic','Chapter_Id','Chapter'])\n",
    "            print('New file started: %s-%s.csv'%(c_row['Chapter_Id'],File_Num))\n",
    "        Chapters.at[c_index,'LastProcessedTopic']=t_row['Topic_Id']\n",
    "        cnt_topics=cnt_topics+1\n",
    "        Chapters.at[c_index,'cnt_ProcessedTopics']=cnt_topics\n",
    "        Chapters.to_csv(Chapters_full_filename, header=True, index=False)\n",
    "        #print('Processed %s messages'%len(t_pd))\n",
    "        #print('--------------------------------------------')\n",
    "        if t_row['Topic_Id']==c_row['StopOnTopic']:\n",
    "            break                  \n",
    "print()\n",
    "print('Processing  complete')            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
