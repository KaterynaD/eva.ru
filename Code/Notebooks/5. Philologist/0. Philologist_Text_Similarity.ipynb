{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "eva.ru forums require registration to publish your posts, but you can do it anonymously. Even your virtual identity is known only to forum administrators and readers cannot connect various anonymous posts to one person. There is an anonymous user with very specific style and topics to discuss. Her posts can be easily identified by a human been.\n",
    "\n",
    "The notebook applies text similarity technique because there are no training data. I can only pre-select long texts with double divider between paragraphs which is a feature of the anonumous author style. But she is not unique in this style.\n",
    "\n",
    "She names herself Philologist in few posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25432,
     "status": "ok",
     "timestamp": 1634681509390,
     "user": {
      "displayName": "Kateryna Drogaieva",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB1LQYJH97Lqm6RsCcAADFgzEAa0YxMUT7P4mjlA=s64",
      "userId": "10877377912113147805"
     },
     "user_tz": 420
    },
    "id": "3I5s9Vdzn5WB",
    "outputId": "a86f9afc-8a4e-413f-bd4e-2979df29d7d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_dtHIFcGoF8V"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UzgfLBm-t9l5"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "byo5RoPmoLVJ"
   },
   "outputs": [],
   "source": [
    "Data = '/content/drive/MyDrive/Colab Notebooks/Projects/eva/Data/'\n",
    "Messages_filename='long_m2.csv'\n",
    "Messages_full_filename=os.path.join(Data, Messages_filename)\n",
    "\n",
    "ts_filename='ts.csv'\n",
    "ts_full_filename=os.path.join(Data, ts_filename)\n",
    "\n",
    "ps_filename='philologist_samples.csv'\n",
    "ps_full_filename=os.path.join(Data, ps_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "94HrRE9Moduk"
   },
   "outputs": [],
   "source": [
    "Messages = pd.read_csv(Messages_full_filename, error_bad_lines=False, index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TxEpjKaZogiE"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ax_kx-2HomcU"
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "nb_path = '/content/drive/MyDrive/Colab Notebooks/Library'\n",
    "sys.path.insert(0,nb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dd4OqfkwonRv"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SKArueKYoppF"
   },
   "outputs": [],
   "source": [
    "df_models=pd.DataFrame({ 'model' : [\n",
    "        'DeepPavlov/rubert-base-cased-sentence',\n",
    "        'sentence-transformers/paraphrase-multilingual-mpnet-base-v2',\n",
    "        'sentence-transformers/paraphrase-xlm-r-multilingual-v1',\n",
    "        'sberbank-ai/sbert_large_nlu_ru'],'column' : ['DeepPavlov','mpnet','xlm-r','sbert']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U3k-IOqbsncQ"
   },
   "outputs": [],
   "source": [
    "samples=[\n",
    "'Я бы не сказала, что здоровье тут фактор. Я болела по 8-9 месяцев в году с 7 до 15 лет. Как только я цепляю вирус, это переходит в затяжной бронхит на 2 месяца. Это не мешает быть электровеником. Моя мотивация всегда была простой. Свернуть горы, чтобы ничего не делать. В 8 лет я знала, что мне нужен диплом кандидата наук и место на кафедре. А потом развалился Союз, и там зарплату по году не платили. Был один такой год в 1990-е... Но при этом туда было не устроиться. Город и область большие. У каждого проректора очередь стоит из всех знакомых дочку туда устроить, чтобы она там заняла стул до гробовой доски на той кафедре. Всю жизнь с кашлем ходила на работу. Теперь в эмиграции делаю прививку от гриппа. Когда Вы сделали все для достижения цели, а потом температура 39, и дышать не можешь... Я так выросла. Я знаю, что это такое с раннего детства. Это не мешает сворачивать горы. У нас кто не член профсоюза, тот никто, и звать его никак, поэтому все электровеники. Когда получаешь ставку и членство в профсоюзе, можно плевать в потолок и ничего не делать. Если завтра объявят ставку, у нас только внутренних кандидатов будет человек сто на контракте. А ставок полно, и их не объявляют. Капиталисты все переводят на временные контракты. Свободные ставки висят.Здоровье - это не препятствие. Антибиотики горстями и побежала на работу. Я так всю жизнь жила. А в Америке начала делать прививки от гриппа. Либо не заболеваю, либо тяжелое состояние только 1-2 дня. Правда, у меня у всех предков гены здоровые. Остальные органы у меня вроде бы должны выдерживать все. В детстве была аллергия, потом астма, потом бронхиты. А по генам там все либо пришмаленные уральцы, которые прыгали в ледяные озера и лазали по горам на уровне международных соревнований и побеждали, либо не уральцы, но тоже жили за 90 и 100 лет... А я слабенькая. Зато я духом сильная. Меня школа воспитала.Бывало, приду после 2 месяцев болезни, напишу контрольную на 5 одна в классе, а мне говорят: \"\"Ну тебя же не было в школе, я не могу тебе поставить 5 в четверти\"\". Украина всегда была Украиной: на примере таких шустрых всегда показывают, что бывает с теми, кто учителю не носит подарки и деньги. У нас и при Союзе советской власти не было. Так что воспитали они из меня электровеника, который учился за всю школу... потому что за него подарки не носили.А так я ленивая. Готова свернуть горы, чтобы ничего не делать. И сотня таких же электровеников на работе дышат мне в затылок. Они тоже все ленивые коровы. Но у нас если не электровеник, то и не дадут те часы. Конкуренция огромная. Электровеником быть мало. Надо суметь угодить всем, кому надо, а остальных столкнуть лбами. И только тогда получишь свой контракт впереди остальных. Каждый день садятся самолеты с новыми иммигрантами. И там далеко не помойка с беженцами. Каждый иранский профессор с 5 поколениями профессоров присылает своих дочек, каждый староста деревни из Пакистана, который ест на золоте, обучил свою дочку в Англии в универе, а там оставить не смог. Они все шлют их к нам. Они не идут с резюме в хвост очереди, как я. Они идут в мечеть и там им дают координаты, к кому подойти. И им впереди всех дают часы. Вынимают из-за кассы супермаркета бывшего проректора из Омана и делают его координатором. И уже он распределяет часы. И он уже выбирает из канадцев с учеными степенями своего человечка из Ливана. Чтобы туда втолкнуться, надо, чтобы в последнюю секунду кто-то не смог выйти, а ты пришла и всех устроила больше всех. Так получается не всегда...Какие болезни? Если я разрешу себе быть больной, то надо уже садиться дома и варить мужу суп. А я суп варить не умею и не хочу. Я лучше буду электровеником. Это легче. Все закрыли рты, мама еле дышит, мама на 10 работах.Здоровье... Как мама моей подружки хотела свою дочку замуж выдать, так она с давлением 200 жарила котлеты всем, кого наметила в жертвы. У нее была цель, она ее добилась. Надо сказать, что она все сделала с большой выгодой для себя. В 18 выдала девку замуж, посадила мужу на шею. И уже они 25+ лет вместе живут. Под мудрым руководством тещи зять послушно выучил английский язык и вывез их всех в Канаду. Ездит молча на работу 2 часа в одну сторону, а тещу врачи вытащили с того света. Украинские врачи ее бы давно отправили на кладбище. Так что жарила котлеты и свое в конечном счете она с этого поимела. А девка тупая, она ничего не может сама. Она может отнести доллары в зачетке или работать руками. Так мама ее выдала замуж так, что все проблемы решил такой зять, который делал то, что придумала теща. Сам он ничего бы не сделал. А теща ему говорила, что делать. Сколько там только фиктивных справок о здоровье было куплено... Это все делала теща.Так что здоровье не является препятствием. Электровеники чувствуют себя плохо, если не добились цели. Поэтому они ложатся трупом и цапают цель зубами. Если повезет, то потом можно наконец ничего не делать.'\n",
    ",'Нет, у меня нет лошадиного здоровья. Я с 7 до 15 лет вообще болела 8-9 месяцев в году бронхитами. У меня хронический бронхит. Ты бежишь и бежишь, а потом у тебя температура 39, и ты выбываешь из гонки. И не можешь дышать.Я живу в Канаде и делаю теперь каждый год прививки от гриппа. Помогает. Было такое, что работала на трех работах 60+ часов в неделю. Потом одно время в 5 конторах преподавала 35 аудиторных часов в неделю. А там в классах, рассчитанных на 25 студентов, сидят 45. Агрессивных и хамоватых. И ср...ть они хотели на всех. Там выживает только преподаватель-спецназ. Я отдаю себе приказ, что я самый мощный спецназ.Когда мне было трудно, я вспоминала, как дедушка взял в руки неуправляемую воинскую часть после войны. Сняли там командира, дедушка ждал демобилизации, у него была мирная специальность, ему не была нужна карьера военного. А его выслали туда - взять в руки неуправляемых подростков, которые свое не дослужили, так как их призвали во время войны. Я не знаю, как он их взял в руки, но взял. И они его обожали. А прежний командир заходил в казарму, а они голые на лавках лежат и командира матом шлют.Вы отдаетет себе приказ и все. То, что империалисты наживаются, да, но у нас на каждый аудиторный час по 100-150 человек в списке учителей на контракте. Мы не можем рассуждать, что там кто-то озолачивается. Это другой аспект проблемы. Да, они озолачиваются. А мне надо быть первым кандидатом из 100 человек, иначе мои часы получит другой. Иначе я не получу ничего. Или надо уходить в другую сферу.Я дома в Украине была легкоранимым созданием. Ходила на работу 15 минут пешком, а потом приходила домой, а нянька моего ребенка мне уже сварила суп и сделала заливную рыбу. В Канаде так нельзя. Мы тут рвем зубами на куски всех, кого видим.Я тоже многого избегаю. Я не мою себе полы и унитазы. Это делают другие иммигрантки. Я не работаю руками. Я только вещаю о языке и коммуникации. Я преподаю. Да, у меня может быть 43 агрессивных придурка в классе и 3 нормальных отличника. Но я же не работаю руками.Зато у нас дом за полмиллиона в белом районе. На нашей улице еще ни одной смуглой рожи нет. Я летаю на Карибы плавать в море. Да, в Украине я жила спокойно. Потом летом ехала на такую базу отдыха, где туалет на улице, и комары кусают в зад. Я могу спокойно полелеть на недельку в Париж или в Испанию. Да много чего... Возможностей много.А потом надо заткнуться и идти выдирать свои часы впереди 100 таких же профессоров, которые приперлись из Пакистана, бывшего соцлагеря или Ирана. И опускать шелковый платок секретарше в сумочку быстрее них. И слушать, кто из них кому что стукнул и кого вылизал. И смотреть в оба глаза, кто с кем в туалет пошел. И тогда может быть выдерешь свои часы. А потом надо, чтобы дебилы не вынесли меня вперед ногами из того класса. Чтобы они написали в анкете, что я им понравилась, а коза из Румынии не понравилась. Чтобы ее часы потом подгребла я. Потому что я умею быть первой. Не сказала бы, что оно мне нравится. Я люблю висеть в гамаке с книжечкой. Но я умею умереть за достижение цели. Ракета наведена на цель. Я ракета. У цели нет шансов, если меня навели на цель правильно. Другие ракеты упадут, а я далечу. Я всегда добираюсь до цели, если она поставлена правильно. Я один раз не достигла цели, но ее не достиг никто. Эту ставку никому так и не дали.Не надо было мне идти преподавать английский. Тут да, большие ставки, но ставку сейчас никому не дают. Люди выходят на пенсию, а новых не нанимают. Мы все рвем друг друга на куски за каждый час в контракте.Меня не интересует, выдержу я или нет. Я вижу только цель. Цель достигнута - я чувствую себя хорошо. Я вылезла на головы 100 конкурентов, когда у нас освободилась ставка. Мне дали и стол этого человека, и отдельный номер телефона. А ставку не дали. Никому не дали. Вообще перестали нанимать. И забастовки были, все было. У нас сейчас полно свободных ставок, а капиталисты хотят иметь нас во все дыры на контракте.Вообще я лентяйка. Я готова свернуть горы, чтобы ничего не делать. Тот, кто получает ставку и членство в профсоюзе ничего наконец не делает. Раньше по 10-15 лет ставку ждали. А теперь их просто никому не дают.В Америке официанты бегают по 12 часов на ногах. И с улыбкой на морде. Никого там не интересует, какой у Вас варикоз. Люди заканчивают университеты с огромными долгами и бегают 12-часовые смены. Если повезет, станет барменом. Бармен в пятницу и субботу в обычном баре имеет наличкой чаевых 1000 долларов за смену. Человек с МВА просидит день в офисе, потом бежит в ресторан. Им там некогда думать, что они могут, чего не могут. Они рвут зубами все, что можно.Я своему мужу сделала студенческую визу в Америке. Он ночью в компьютерной компании сидел ночные смены на компьютерной безопасности по 12 часов, потом дремал в метро и ехал в колледж ради визы. Чтобы мы не выпали из легального поля. И без сна там сидел, чтобы у нас всех была виза. Чтобы мы из Штатов перескочили на канадскую эмиграцию по короткой очереди из Америки. А спал он уже потом, после колледжа. Ничего, пережили. Приехали в Канаду. Сидит теперь в огромном кабинете и щеки надувает. По Украине не скучает. Он украинец, поэтому Украина ему до лампочки. А я русский шовинист. Мы, русские, вложили туда много. Надо Украину зачистить и навести там порядочек...Но лучше я пока в Канаде буду трудоголиком. Зачистка Украины - не моя специализация. Я лучше буду вещать про согласование времен и множественное число.Я тоже ненавижу толпу людей. Я теряю энергию от людей. Но я вынуждена отдавать себе приказ и играть концерт. Это все же лучше, чем я бы сейчас сидела в Украине и продолжала щебетать на мове, как все чудесно. Мне надо было ребенка пристроить в стране чистых туалетов и зеленых лужаек.Жалко, конечно, что так Украину загадили. Это наш, русский проект. Мы там уже оставили столько жизней, столько построили, столько вложили. Что мы, интеллигенты, можем... Разве что самим себе отдать приказ. Нажал на кнопку и высадился с парашютом. Я нажала кнопку эвакуации в Канаду.Наши сильные стороны тут никого не волнуют. Это дорогая страна, нужен непрерывный поток денег. И много часов, чтобы летом, пока часов нет, получать на халяву большую страховку от безработицы. В мае-августе можно ничего не делать и получать приличную страховку. Или делать такие халтуры, которые официально оформят потом. Раньше я летом пахала. Теперь вот решила плюнуть и 4 месяца не работать, а делать свои дела. Другие.Короче, мы крутимся, как белки в колесе.'\n",
    ",'Я не еврейка, так что объясняю преимущества. С евреями все понятно: они всю историю то в одной стране что-то доили, то в другой. Русским положено жить в своей империи. Но мне надоело развлекать Хохляндию, поехала и я по миру покататься.Они открывают двери своим детям. Насколько эти дети смогут этим воспользоваться... Там чистенько, стабильненько, можно быть скромным профессором статистики под Вашингтоном и жить в шикарном доме, а испаноязычная прислуга его будет отмывать. Ну, это вариант пристроить так детей. Или сделать из них американских стоматологов. Или иммиграционных юристов.Все зависит от знаний родителей или консультантов, которые помогут создать сюжет.Или попроще - сделать из деточки бухгалтера и посадить его куда-нибудь в федеральный музей зарплату начислять. Чисто, стабильно, культурно.Ха! Или сидит себе иммигрантское чадо в Библиотеке Конгресса. Кофту на стул с утра повесила и свалила на весь день клиентам показывать недвижимость. Федеральная зарплата небольшая, но все отпуска, льготы и страховки. А зарабатывает она на недвиге.Это дети иммигрантов.А сами иммигранты... Ну в чем проблема? Пошел в аспирантуру на 4-5 лет. А лучше муж с женой одновременно. На каждого по 2000 долларов в месяц аспирантской стипендии получат - можно жить. А лет через 5 у них уже работа не бей лежачего. Конечно, сейчас уже весь мир проснулся, каждый пакистанец своих дочек присылает, каждый иранец тренирует деточек зубами откусывать свой кусок американского пирога.А почему не взять что-нибудь и себе в карман не положить?Конечно, арабы лидируют в том смысле, что у них 3-5 детей - они больше ухватят. Если русские пристроят только одного ребенка, то прибыль небольшая. А посмотрите на малограмотных палестинок, как они доят ту Америку с Канадой. И куда прут их дети. И сколько они там откусили.Люди это все видят и думают, что чего лениться, пора и им поучаствовать.Нда... Бруклин... Вы отстали от жизни. В Бруклине нынче шикарно. Сейчас народ на Стейтен Айленде селится. Два часа автобус идет оттуда до Манхэттена. Но ничего, это не мешает детям иммигрантов развиваться.Это богатые страны. Там можно что-то ухватить. Одесские спекулянтки и парикмахерши вывезли не особо способных дочек, но дочки те отлично сели на Уолл-Стрит на хорошие стулья. Им мама с папой рассказали, что надо не ушами хлопать, а идти туда, куда надо. Интеллигентки тоже хотят свои крошки от пирога. А те, кто не интеллигент, то у них вообще ясные подходы: страны богатые - они откроют бизнес и будут доить клиентов.Люди берут в банке кредит на 80 тысяч долларов и заказывают ремонт подвала. Это все в долг. А шустрые арабы эти бабки получают. А потом они нанимают вчера приехавшую шваль, и шваль делает ремонт. Так что только ленивые интеллигенты до сих пор сидят в Бердичеве и ждут у моря погоды. Пишут методичку по статистике на украинском языке, продают ее студентам. А смысл? Давно устроились бы профессорами статистики где-нибудь ну не в Стэнфорде, да, попроще...Все сложно. Однозначных ответов нет. Жизнь показывает, что у кого была четкая стратегия, то люди устроились замечательно. У кого не было стратегии - те кругами ходят, но тоже хорошо устроены. В каком-то смысле, приключений полно, жизнь совсем не скучная. Ну, сидели бы они дома. Сколько можно сидеть? Седина в бороду - бес в ребро. Полетел за океан и начал там отжигать.Химики стали медсестрами, кардиологи - техниками прибора ЭКГ, грамматисты преподают английскую стилистику, спецы по французской стилистике обучают англофонов, спецы по английской - франкофонов. Инженеры чертят какие-нибудь купола для церквей. Архитекторы ходят на бесконечные почасовки, но тоже живут сытно.Физики двинули в бухгалтеры. Кандидаты наук по физике доставляют пиццу. Не без того, конечно, чтобы кто-нибудь не страдал. Каждый развлекается так, как у него получается.Я одно время в один день 10 часов в трех конторах преподавала. Но я же не руками работала. Языком трепала, как обычно. А потом летом чик - и на страховку по безработице. Да, устала, но потом же отдохнула. Лежишь себе на заднем дворе и в небо смотришь. Хорошо.Спецы из Ирана по сравнительному литературоведению дома не ходят отмывать и хлеб не выпекают. Они идут в аспирантуру и начинают сразу клепать диссертацию по бизнесу. Поэтому доценты психологии из России видят, как это сделать, и идут за ними следом. А доценты в РФ качественные. Психологию они учили. Поэтому они манипулируют всеми в универе профессионально. Если иранки и арабки заносят что надо и кому надо через мечети и получают свои часы и свои стипендии, то российские психологи и социологи имеют во все дыры всех, кого хотят. Они быстро и каждую секретаршу завкафа, и того завкафа прибирают к рукам. И получают то, что хотят.Ленивые долго спят и дремлют, но потом тоже смотрят, что и как. И учатся. И делают выводы. И берут пример. Кто-то доит еврейскую тему, кто-то - диаспорных украинцев, а кто-то еще что-то изобретает, с кого и что поиметь. Каждый развлекается в меру своих талантов и умения повторить опыт успешных товарищей.Кто совсем ничего не придумал, тот продает страховки или становится агентом по недвиге. Кто мозгами шевелить ленится, тот разрушает старые подвалы и веранды или отмывает что-нибудь хлоркой. Или становится инспектором по плесени. Во влажном климате клиентов полно. Лезет с фонариком по лестнице и проверяет чердаки. Не голодает. Может, плесень - тоже интересное явление. С научной точки зрения. Хм. Каждый устроился так, как у него получилось.Музыканты преподают в школе, а в выходные играют на оргАне в церкви, а вечером - на пьянках. Наличка течет мимо налогов рекой. С ностальгией вспоминают, как в 1990-е играли на пьянках у бандитов. Теперь играют в церкви. Или в доме престарелых на музыкальной терапии. Кирпичи они не кладут и машины не моют. Лодырь на потолок залезет, а руками работать не будет. Кур резать на птицефабрике он не пойдет.Все копируют друг друга. Списывают сюжеты и стратегии. В интернете смотрят, что и где происходит. Если престарелый полковник из какой-нибудь Бурунди может написать в Канаде диссертацию по маркетингу или про экономику Конго, а потом пристроиться преподавать в техникуме в Калифорнии, то почему московский программист не может? Он тоже может. Все соревнуются в том, что еще можно учудить. Или из нефтянки какой-нибудь 50-летний инженер приедет из Коми. Чем он хуже полковника из Бурунди или учителя французской литературы из Конго? Он тоже может пойти на МВА. А там принюхается и найдет свой путь. Чутье его не подведет.'\n",
    ",'Если Вы преподаватель английского, то Вам по силам открыть учебные планы в Интернете. Там везде бесконечный коммьюникейшен. Обычно с упором на райтинг. Как написать отчет, как написать отчет по конкретной специальности, как написать деловое письмо, а типов этих писем безумное количество. Информационное письмо; письмо, что Вам должны деньги, письмо с хорошей новостью, письмо с плохой новостью и так бесконечно.Откройте учебники по деловому английскому. Мы обучаем, например, как написать 5 типов писем по теме \"\"Вы должны нам деньги, пора заплатить\"\". Это огромная индустрия. Всех учат писать. И сантехника учат, и медсестру учат, и ассистента юридической конторы. Писать они не могут. У одних одни проблемы, у других - другие.Учим, как правильно употреблять однородные члены предложения. Вытираем тупорылых мордой об асфальт, черкаем все и отправляем переписывать. Или наоборот, рисуем хорошие баллы, которые они не заслужили. В каждой конторе свой концерт.Везде по-разному. Вы на минуточку еще забыли про канадских франкофонов. Там вообще по три ошибки в слове. У кого нет 3 ошибок в одном слове, так там все грамматические структуры покалеченные. Учим все тому же. Страдательный залог, перфектный инфинитив после модальных глаголов. Да, они щебечут красиво. А что мне их щебетание, если вместо глагола иметь в перфектном инфинитиве они пишут предлог ОВ - ? Они слышат ОВ и пишут его вместо хэв. Это же бред. Вы себе только представьте этот пейзаж. Модальный глагол, потом ОВ, потом причастие прошедшего времени. Такие письма можно куда-то послать? Они не могут без ошибок написать почтовый индекс. Там со справками все о дислексии, буквы путают. Континент малограмотных.А еще прелесть какая: буква эйч в начале слова отсутствует. Франкофоны же. Окончаний в конце слов нет. Они же привыкли, что в их родном языке они не читаются. Да все, что угодно. Малограмотные. Ликбез местным делают иранки, румынки, польки и далее по списку.Презентациям обучаем. Смотреть в глаза аудитории просим. Снижаем баллы за то, что читают по бумажке или пялятся в потолок.'\n",
    ",'У вас всех слишком много стереотипов.Например, что английский в англоязычных странах изучают только иммигранты.Вы учебники-то откройте. Письмо - это индустрия. Обучаем каждого сантехника и каждого врача, как написать 5 разных видов писем \"\"Вы должны нашей компании деньги\"\". От вежливого намека до последнего предупреждения. Обучаем, как писать письмо с плохой новостью. Обучаем, как сообщить хорошую новость.Рутинное письмо учим писать.Информационное письмо.Тех писем безумное количество.Обучаем в одном курсе, как писать заумными словами, а в другом - как избегать сложных слов и писать простыми. Только что нарисовали ему наконец тройку за курс с книжными словами и отправили его на курс, где учат писать простыми. И за все, чему вчера научили, будем снижать оценки. И будем проповедовать, как избегать латинизмов, какие суффиксы теперь ни-ни, потому что это все сложные слова. Надо теперь писать просто.Это индустрия. Арабки и иранки отлично преподают местным английский язык. Чем хуже его знают, тем лучше ставят класс раком. Отлично снижают оценки. А местные тоже сплошь малограмотные. Одни покалеченные грамматические структуры. Рынок большой. Учителей много.А справки о дислексии... Им же запрещено снижать оценки за ошибки в буквах. Они же инвалиды. А у кого справка, что он агрессивный, то надо в специальный офис отнести его тест, чтобы он его в отдельной комнате писал. Там тоже сидит куча чиновников. Принимает у него экзамен - письменный отчет по установленному образцу.Целая индустрия - образование. Работу не нашел - снова к нам пришел. Мы ему новый курс райтинга назначили. Вчера был райтинг для медсестры, а сегодня - для диспетчера какой-нибудь службы. И снова тройки рисуем.Только она закончила курс по пиару, так и работает на бензозаправке, как раньше. Поумнела, пришла на курс \"\"Социальный работник, работа с престарелыми\"\". Мы ей по программе для социальных работников новый курс назначаем. По пиару был другой. Ха ха. Давайте нам зарплату за то, что мы терпим этих тупорылых. Код курса другой, старый уже не подходит. Или программу за это время поменяли. Раньше она учила курс, как писать письмо про хорошую новость, а теперь в новом курсе надо описать плохую весть. Гуд ньюс, бэд ньюс. Кстати, там нужен дефис между гуд и ньюс перед словом \"\"письмо\"\" - \"\"гуд-ньюс письмо\"\". Они этого не знают. Тут-то мы им баллы и снизим. За дефис.Индустрия. В каждой стране свои порядки. Я так много нового узнала про райтинг... Я очень мало преподавала иммигрантам. У нас в городе это блатные места с хорошей зарплатой, туда не попадешь. Я преподаю сложные курсы. Там и дефисы, и всякие бюрократические стандарты. Вот в это пишем тут, а вот это в другом абзаце. Это надо знать, как писать и как преподавать. Паблик спикинг. Отвели взгляд в потолок - минус баллы. Надо смотреть в глаза. Учим смотреть в глаза и руки пожимать. Учим, что индейцам нельзя смотреть в глаза. Там это оскорбительно. Нельзя пойти к индейцам и там смотреть им в глаза. Везде свои правила презентаций. Нельзя заявиться к вождю и требовать, чтобы он вам в глаза смотрел. Так и долбим правила, требуем, чтобы они все выучили.'\n",
    ",'Здесь просто необходимо все скрывать. Все играют роли. А ненависть на рабочем месте хлещет аж будь здоров. Просто есть правила игры.А так да, фальшивые улыбки до последнего момента.Так, как русские, себя ведут в Америке афроамериканцы. Был у меня когда-то директор... афроамериканец в одном нон-профите, взял меня на практику. По его физиономии было всегда видно, что он думает. Отличный парень. Платил мне по 500 долларов в месяц стипендии ни за что, я на них купила в Украине квартиру, потому продала ее в 10 раз дороже в долларах, когда цены поднялись,Мне эти все фальшивые улыбки до лампочки. Знаем мы им цену. Ну, грубоватый начальник был. Но я там ничего не делала, а потом получила за квартиру в 10 раз больше. Выгодно вложила. Им вообще было все равно, кого нанять. В грант вписали практикантов, потом искали их. Чтобы бабки хоть куда-то списать.У нас за 10 минут до конца рабочего дня могут прийти с охраной из отдела кадров и сократить. И запросто. Что таких, у кого 40 тысяч долларов в год, что таких, у кого 150 тысяч в год. И улыбаются им дешевно до последней секунды.А потом - письмо о сокращении в зубы, и охрана выводит на парковку.Звериный оскал капитализма еще никто не отменял.И меня так сокращали с теплых мест, и людей вокруг сокращали.До последнего момента не говорят. Например, у меня в Штатах был контракт: либо они предупреждают за 2 недели, либо без предупреждения на выход и платят за эти 2 недели.Было 5 минут до конца рабочего дня, вызвали и вручили конверт. И дали 5 минут на сборы. Будешь медленно шевелиться - вещи вышлем почтой.Но у меня на глазах там уже так сокращали людей. Я уже это видела. И знала, как это бывает. Хотя на предыдущей работе меня предупредили за месяц и водили в ресторан утешать. Сфера - редакция. Пачками сокращали людей... Было 5 уровней проверки текста, а оставили один. Деньги закончились. Читатель все равно не оценит. Были деньги - делали красиво, закончились деньги...А так да... все душевно, можете собачку свою из дома принести, по пятницам все одеваем джинсы, бублики за счет работодателя.Мы просто играем концерт фальшивыми улыбками. Арабы и иранцы лучше всех играют. Такие сладкие, восточные сладости. Вылизывают всех на всякий случай.Потом воткнут тебе кинжал в сердце, рука не дрогнет. Ха ха! У иранцев в поэзии, говорят, женщина должна быть способна поднять меч. Это Вам не коня на скаку... У них требования в культуре - подними меч. И иди вперед. Прикормят пахлавой и проткнут насквозь. Ненавидят сначала местных, потом всех остальных, потом своих, потому что все конкуренты за каждый доллар.Так и живем в межнациональных коллективчиках. Учимся играть в игры. Улыбаемся так сладко...Я вообще солнышко.Поговорка есть в английском языке: ты никогда не одет полностью, пока не одел улыбку.У меня репетитор была много лет назад по интонации. Мы с ней эту поговорку интонировали до совершенства...Одеваем улыбочки. Одновременно с процессом открывания дверей. Это включено в зарплату. Все ведут себя красиво. Все следят, все стучат. ЛЮБОЕ слово будут использовать против тебя. Конкуренция высокая. И каждому нужно подружку устроить на работу.А русские да... главное - шпилечку аккуратно вставить. А смысл? Что мы с этого поимеем? Ничего. В западной культуре делают то, что приносит бабки. Улыбайся и тряси бабки с лоха. Не надо никому вставлять шпилечки.Работала я когда-то у хозяина бизнеса - иранца. Консультировала его по маркетингу. Она 3 месяца в молодости был потерян в горах, не надеялись выйти, когда была война с Ираком. Не надеялся уже живым выйти... А потом уехал в Америку. А потом прихожу на другую работу, а там иранец и иракец рядышком за столами сидят. Получают бабки из одной конторы и так дружно друг другу улыбаются...Я прошла ту еще школу фальшивых улыбок. Мы никогда не знаем, кто пойдет в гору. Арабы вылизывают всех даже своего уровня. Не только начальника. Они делают ставку на тех, кто потенциально может пойти вверх. Это надо видеть и читать, какой у людей талант что-нибудь найти или придумать, а потом за это Вас поблагодарить. И вылизывают по полной программе. Ты такая золотая, бью челом, благодарен.Да что сказать... Там с молоком матери: лижи зад вождю, топчи тех, кто ниже тебя. Но самые умные выстраивают горизонталь: делают ставку на всех, кто на одном уровне с ним. А уж кто побывал под бомбежками Багдада... Ой, лизать умеют. Они видели ужасы. И хотят пристроиться еще лучше.'         \n",
    "]\n",
    "samples_ids=[\n",
    "97514098,\n",
    "97496707,\n",
    "97883834,\n",
    "97887677,\n",
    "97887700,\n",
    "98972998\n",
    "]\n",
    "df_samples = pd.DataFrame({'Sample_Id':samples_ids,'Sample':samples})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "roN6L3EDpgOi"
   },
   "outputs": [],
   "source": [
    "def similarity_search(pdf_samples):\n",
    "  ldf_ts = pd.DataFrame(columns=df_samples.columns.tolist()+Messages.columns.tolist()+['model','score'])\n",
    "  print(datetime.datetime.now())\n",
    "  for s_index, sample_row in pdf_samples.iterrows():\n",
    "    sample=sample_row['Sample']\n",
    "    print(sample[0:100])\n",
    "    for model_index, model_row in df_models.iterrows():\n",
    "      model_name=model_row['model']\n",
    "      column_name=model_row['column']\n",
    "      print('Processing messages using %s model'%model_name)    \n",
    "      model = SentenceTransformer(model_name)\n",
    "      score=list()\n",
    "      df_ts_tmp = pd.DataFrame(columns=df_samples.columns.tolist()+Messages.columns.tolist()+['model','score'])\n",
    "      for index,row in Messages.iterrows():\n",
    "        embeddings = model.encode([sample,row['message']])\n",
    "        cs = cosine_similarity([embeddings[0]],[embeddings[1]])\n",
    "        score.append(cs.tolist()[0][0])\n",
    "      df_ts_tmp['Message_Id']  = Messages['Message_Id'].tolist()\n",
    "      df_ts_tmp['author']  = Messages['author'].tolist()\n",
    "      df_ts_tmp['message']  = Messages['message'].tolist()\n",
    "      df_ts_tmp['model'] = column_name\n",
    "      df_ts_tmp['score'] = score\n",
    "      df_ts_tmp['Sample_Id']=sample_row['Sample_Id']\n",
    "      df_ts_tmp['Sample']=sample_row['Sample']\n",
    "      ldf_ts=ldf_ts.append(df_ts_tmp)\n",
    "      print('Processing complete')\n",
    "  print(datetime.datetime.now())\n",
    "  ldf_ts.to_csv(ts_full_filename, header=True, index=False)\n",
    "  return ldf_ts   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UvwcmVNL2YrE"
   },
   "outputs": [],
   "source": [
    "df_ts = similarity_search(df_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9rhCWnn0rkND"
   },
   "outputs": [],
   "source": [
    "#df_ts = pd.read_csv(ts_full_filename, error_bad_lines=False, index_col=False, usecols=['Sample_Id','model','score','Message_Id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2mDfUBuFagYk"
   },
   "outputs": [],
   "source": [
    "def get_samples(n_samples):\n",
    "  Message_Ids=list()\n",
    "  for s_index, sample_row in df_samples.iterrows():\n",
    "    Sample_Id=sample_row['Sample_Id']\n",
    "    IDs=list()\n",
    "    for model_index, model_row in df_models[df_models['column'].isin(['DeepPavlov','sbert'])].iterrows():\n",
    "      model_name=model_row['model']\n",
    "      column_name=model_row['column']\n",
    "      IDs.append(df_ts[(df_ts['model']==column_name) & (df_ts['Sample_Id']==Sample_Id)].sort_values(['score'], ascending=False)[0:n_samples+1:1]['Message_Id'].tolist())\n",
    "      Sample_Message_Ids=list()\n",
    "    for Id in IDs:\n",
    "      if len(Sample_Message_Ids)==0:\n",
    "        Sample_Message_Ids=Id\n",
    "      else:\n",
    "        Sample_Message_Ids=list(set(Sample_Message_Ids) & set(Id))\n",
    "    Message_Ids=list(set(Message_Ids+Sample_Message_Ids))\n",
    "  return Message_Ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2F9ntr9G40G-"
   },
   "outputs": [],
   "source": [
    "df_samples = Messages[Messages['Message_Id'].isin(Message_Ids)][['Message_Id','message']]\n",
    "df_samples.columns=['Sample_Id','Sample']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AnKaLY_j6bHz"
   },
   "outputs": [],
   "source": [
    "df_samples = df_samples[df_samples['Sample_Id'] != 68039009]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 166,
     "status": "ok",
     "timestamp": 1634166555704,
     "user": {
      "displayName": "Kateryna Drogaieva",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB1LQYJH97Lqm6RsCcAADFgzEAa0YxMUT7P4mjlA=s64",
      "userId": "10877377912113147805"
     },
     "user_tz": 420
    },
    "id": "UfQJN3r5yDA3",
    "outputId": "2086d311-a5a4-4390-8de1-b076f5ea2fb7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample_Id</th>\n",
       "      <th>Sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>97328387</td>\n",
       "      <td>А Вы бы не работали по 8 часов, а защитили бы ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>97496707</td>\n",
       "      <td>Нет, у меня нет лошадиного здоровья. Я с 7 до ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>97514098</td>\n",
       "      <td>Я бы не сказала, что здоровье тут фактор. Я бо...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>97677364</td>\n",
       "      <td>Так мне никто не платит за то, чтобы я этот те...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>97883834</td>\n",
       "      <td>Я не еврейка, так что объясняю преимущества. С...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>97887677</td>\n",
       "      <td>Если Вы преподаватель английского, то Вам по с...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>97887688</td>\n",
       "      <td>Успокойтесь. Я выросла среди евреев. Так что н...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>97887700</td>\n",
       "      <td>У вас всех слишком много стереотипов.Например,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>98479202</td>\n",
       "      <td>Ой, ну что же вы такие слабые пошли, отличницы...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>98758326</td>\n",
       "      <td>Автор может переехать за границу. Оттуда оплач...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553</th>\n",
       "      <td>98764959</td>\n",
       "      <td>Да ладно вам, наивным, про 8 лет. Я в 8 лет ре...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>556</th>\n",
       "      <td>98829531</td>\n",
       "      <td>Чего такой спор? Учеба в Канаде РАЗНАЯ. Одно д...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>98972998</td>\n",
       "      <td>Здесь просто необходимо все скрывать. Все игра...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>99055138</td>\n",
       "      <td>Это у вас в Америке можно устроиться историком...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td>99537597</td>\n",
       "      <td>А я в 8 лет приняла решение любой ценой получи...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>100095387</td>\n",
       "      <td>А в чем заблуждение? Мне на работе присылали т...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>100316193</td>\n",
       "      <td>А Вы работаете в отделе кадров и проверили каж...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776</th>\n",
       "      <td>101303741</td>\n",
       "      <td>У нас очень много крутых спецов. В кого ни плю...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777</th>\n",
       "      <td>101306119</td>\n",
       "      <td>Мы снижаем оценку за глагол ютилайз и требует ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788</th>\n",
       "      <td>101447298</td>\n",
       "      <td>Все иностранцы, которые изучают русский язык, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>792</th>\n",
       "      <td>101479281</td>\n",
       "      <td>Если бы я на такое обращала внимание, я бы не ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>801</th>\n",
       "      <td>101560735</td>\n",
       "      <td>Угу, вторую диссертацию пишет - точно малообра...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>102089984</td>\n",
       "      <td>Я живу в обществе, где вообще никто никому не ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>102538016</td>\n",
       "      <td>Вы в другой категории. Вы потомок шведов, кото...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1160</th>\n",
       "      <td>97930804</td>\n",
       "      <td>Да, это серьезный удар по мозгам. Диалоги. Кра...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1163</th>\n",
       "      <td>97981431</td>\n",
       "      <td>Итак, пейзаж. Представим себе курс, имеющий от...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1184</th>\n",
       "      <td>98571031</td>\n",
       "      <td>Вот это я зажгла... Столько откликов... видимо...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1206</th>\n",
       "      <td>98802544</td>\n",
       "      <td>Никакие блоги я бесплатно не веду. За деньги к...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209</th>\n",
       "      <td>98790603</td>\n",
       "      <td>Как мне нравится картинка. Отличная картинка.Д...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211</th>\n",
       "      <td>98802530</td>\n",
       "      <td>Как потомственная отличница в энном поколении ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1227</th>\n",
       "      <td>98974560</td>\n",
       "      <td>Делаем ставки! Не факт, что он получит то, что...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1262</th>\n",
       "      <td>99341751</td>\n",
       "      <td>Ха ха! Вот насмешили меня в соседней теме. Гов...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1296</th>\n",
       "      <td>99621948</td>\n",
       "      <td>Повеселили. Тут можно измерять бесконечно. Год...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1331</th>\n",
       "      <td>100006697</td>\n",
       "      <td>А вы готовьтесь. Приготовьте наручники и дубин...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1356</th>\n",
       "      <td>100236024</td>\n",
       "      <td>Вы МНЕ будете рассказывать, к чему я имею отно...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1363</th>\n",
       "      <td>100260797</td>\n",
       "      <td>Нужно взять персоналии тех, кто критикует, под...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1364</th>\n",
       "      <td>100264490</td>\n",
       "      <td>Это про текст на английском языке. На высоких ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1365</th>\n",
       "      <td>100348442</td>\n",
       "      <td>Учителей они заменят легко. Они не на роботов ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1408</th>\n",
       "      <td>100530826</td>\n",
       "      <td>Если вы помните те времена, когда в СССР возле...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1409</th>\n",
       "      <td>100537245</td>\n",
       "      <td>Да уж реальнее некуда. Написано, конечно, чере...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1465</th>\n",
       "      <td>100984216</td>\n",
       "      <td>Ха! Это Вы еще не видели моих коллег, если я В...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1471</th>\n",
       "      <td>101053983</td>\n",
       "      <td>СтереотипыВы слишком погрузились в стереотипы....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1487</th>\n",
       "      <td>101117528</td>\n",
       "      <td>Меня когда сократили в американской редакции, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1491</th>\n",
       "      <td>101124020</td>\n",
       "      <td>Чтобы писать более утонченным слогом, нужно ВР...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1571</th>\n",
       "      <td>101935083</td>\n",
       "      <td>Смотря что и как под этим заголовком преподава...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1572</th>\n",
       "      <td>101935146</td>\n",
       "      <td>&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;Вы серьезно думаете, что общеобразова...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1574</th>\n",
       "      <td>101936264</td>\n",
       "      <td>Так это же сравнение.ОК. На первый взгляд каже...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1576</th>\n",
       "      <td>101939931</td>\n",
       "      <td>С другой стороны, родители сдали своих детей в...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1583</th>\n",
       "      <td>101976410</td>\n",
       "      <td>Вы просто говорите так, как нужно говорить. В ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1611</th>\n",
       "      <td>102290403</td>\n",
       "      <td>Очень много вредительства в том, что детям вну...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1612</th>\n",
       "      <td>102295735</td>\n",
       "      <td>А КТО должен оплатить расходы на все ваши вари...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1613</th>\n",
       "      <td>102295797</td>\n",
       "      <td>Все понятно, почему такая программа. В России ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1614</th>\n",
       "      <td>102295858</td>\n",
       "      <td>В каждой стране свои возможности в плане образ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1633</th>\n",
       "      <td>102543404</td>\n",
       "      <td>Автор хочет слышать. Так не бывает. Вы услышит...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sample_Id                                             Sample\n",
       "430    97328387  А Вы бы не работали по 8 часов, а защитили бы ...\n",
       "446    97496707  Нет, у меня нет лошадиного здоровья. Я с 7 до ...\n",
       "447    97514098  Я бы не сказала, что здоровье тут фактор. Я бо...\n",
       "466    97677364  Так мне никто не платит за то, чтобы я этот те...\n",
       "472    97883834  Я не еврейка, так что объясняю преимущества. С...\n",
       "473    97887677  Если Вы преподаватель английского, то Вам по с...\n",
       "474    97887688  Успокойтесь. Я выросла среди евреев. Так что н...\n",
       "475    97887700  У вас всех слишком много стереотипов.Например,...\n",
       "521    98479202  Ой, ну что же вы такие слабые пошли, отличницы...\n",
       "552    98758326  Автор может переехать за границу. Оттуда оплач...\n",
       "553    98764959  Да ладно вам, наивным, про 8 лет. Я в 8 лет ре...\n",
       "556    98829531  Чего такой спор? Учеба в Канаде РАЗНАЯ. Одно д...\n",
       "570    98972998  Здесь просто необходимо все скрывать. Все игра...\n",
       "581    99055138  Это у вас в Америке можно устроиться историком...\n",
       "635    99537597  А я в 8 лет приняла решение любой ценой получи...\n",
       "679   100095387  А в чем заблуждение? Мне на работе присылали т...\n",
       "701   100316193  А Вы работаете в отделе кадров и проверили каж...\n",
       "776   101303741  У нас очень много крутых спецов. В кого ни плю...\n",
       "777   101306119  Мы снижаем оценку за глагол ютилайз и требует ...\n",
       "788   101447298  Все иностранцы, которые изучают русский язык, ...\n",
       "792   101479281  Если бы я на такое обращала внимание, я бы не ...\n",
       "801   101560735  Угу, вторую диссертацию пишет - точно малообра...\n",
       "856   102089984  Я живу в обществе, где вообще никто никому не ...\n",
       "893   102538016  Вы в другой категории. Вы потомок шведов, кото...\n",
       "1160   97930804  Да, это серьезный удар по мозгам. Диалоги. Кра...\n",
       "1163   97981431  Итак, пейзаж. Представим себе курс, имеющий от...\n",
       "1184   98571031  Вот это я зажгла... Столько откликов... видимо...\n",
       "1206   98802544  Никакие блоги я бесплатно не веду. За деньги к...\n",
       "1209   98790603  Как мне нравится картинка. Отличная картинка.Д...\n",
       "1211   98802530  Как потомственная отличница в энном поколении ...\n",
       "1227   98974560  Делаем ставки! Не факт, что он получит то, что...\n",
       "1262   99341751  Ха ха! Вот насмешили меня в соседней теме. Гов...\n",
       "1296   99621948  Повеселили. Тут можно измерять бесконечно. Год...\n",
       "1331  100006697  А вы готовьтесь. Приготовьте наручники и дубин...\n",
       "1356  100236024  Вы МНЕ будете рассказывать, к чему я имею отно...\n",
       "1363  100260797  Нужно взять персоналии тех, кто критикует, под...\n",
       "1364  100264490  Это про текст на английском языке. На высоких ...\n",
       "1365  100348442  Учителей они заменят легко. Они не на роботов ...\n",
       "1408  100530826  Если вы помните те времена, когда в СССР возле...\n",
       "1409  100537245  Да уж реальнее некуда. Написано, конечно, чере...\n",
       "1465  100984216  Ха! Это Вы еще не видели моих коллег, если я В...\n",
       "1471  101053983  СтереотипыВы слишком погрузились в стереотипы....\n",
       "1487  101117528  Меня когда сократили в американской редакции, ...\n",
       "1491  101124020  Чтобы писать более утонченным слогом, нужно ВР...\n",
       "1571  101935083  Смотря что и как под этим заголовком преподава...\n",
       "1572  101935146  >>>>>>>>>Вы серьезно думаете, что общеобразова...\n",
       "1574  101936264  Так это же сравнение.ОК. На первый взгляд каже...\n",
       "1576  101939931  С другой стороны, родители сдали своих детей в...\n",
       "1583  101976410  Вы просто говорите так, как нужно говорить. В ...\n",
       "1611  102290403  Очень много вредительства в том, что детям вну...\n",
       "1612  102295735  А КТО должен оплатить расходы на все ваши вари...\n",
       "1613  102295797  Все понятно, почему такая программа. В России ...\n",
       "1614  102295858  В каждой стране свои возможности в плане образ...\n",
       "1633  102543404  Автор хочет слышать. Так не бывает. Вы услышит..."
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8156513,
     "status": "ok",
     "timestamp": 1634193744740,
     "user": {
      "displayName": "Kateryna Drogaieva",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB1LQYJH97Lqm6RsCcAADFgzEAa0YxMUT7P4mjlA=s64",
      "userId": "10877377912113147805"
     },
     "user_tz": 420
    },
    "id": "2EWp4TNM2nRI",
    "outputId": "82f10889-6b21-4fd1-f1af-abd887d62c01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-14 00:36:42.130891\n",
      "А Вы бы не работали по 8 часов, а защитили бы диссертацию. И получили бы в любом маленьком городишке\n",
      "Processing messages using DeepPavlov/rubert-base-cased-sentence model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/DeepPavlov_rubert-base-cased-sentence. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-multilingual-mpnet-base-v2 model\n",
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-xlm-r-multilingual-v1 model\n",
      "Processing complete\n",
      "Processing messages using sberbank-ai/sbert_large_nlu_ru model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/sberbank-ai_sbert_large_nlu_ru. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Нет, у меня нет лошадиного здоровья. Я с 7 до 15 лет вообще болела 8-9 месяцев в году бронхитами. У \n",
      "Processing messages using DeepPavlov/rubert-base-cased-sentence model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/DeepPavlov_rubert-base-cased-sentence. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-multilingual-mpnet-base-v2 model\n",
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-xlm-r-multilingual-v1 model\n",
      "Processing complete\n",
      "Processing messages using sberbank-ai/sbert_large_nlu_ru model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/sberbank-ai_sbert_large_nlu_ru. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Я бы не сказала, что здоровье тут фактор. Я болела по 8-9 месяцев в году с 7 до 15 лет. Как только я\n",
      "Processing messages using DeepPavlov/rubert-base-cased-sentence model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/DeepPavlov_rubert-base-cased-sentence. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-multilingual-mpnet-base-v2 model\n",
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-xlm-r-multilingual-v1 model\n",
      "Processing complete\n",
      "Processing messages using sberbank-ai/sbert_large_nlu_ru model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/sberbank-ai_sbert_large_nlu_ru. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Так мне никто не платит за то, чтобы я этот текст долго обдумывала и приводила в порядок.И я это пиш\n",
      "Processing messages using DeepPavlov/rubert-base-cased-sentence model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/DeepPavlov_rubert-base-cased-sentence. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-multilingual-mpnet-base-v2 model\n",
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-xlm-r-multilingual-v1 model\n",
      "Processing complete\n",
      "Processing messages using sberbank-ai/sbert_large_nlu_ru model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/sberbank-ai_sbert_large_nlu_ru. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Я не еврейка, так что объясняю преимущества. С евреями все понятно: они всю историю то в одной стран\n",
      "Processing messages using DeepPavlov/rubert-base-cased-sentence model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/DeepPavlov_rubert-base-cased-sentence. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-multilingual-mpnet-base-v2 model\n",
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-xlm-r-multilingual-v1 model\n",
      "Processing complete\n",
      "Processing messages using sberbank-ai/sbert_large_nlu_ru model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/sberbank-ai_sbert_large_nlu_ru. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Если Вы преподаватель английского, то Вам по силам открыть учебные планы в Интернете. Там везде беск\n",
      "Processing messages using DeepPavlov/rubert-base-cased-sentence model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/DeepPavlov_rubert-base-cased-sentence. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-multilingual-mpnet-base-v2 model\n",
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-xlm-r-multilingual-v1 model\n",
      "Processing complete\n",
      "Processing messages using sberbank-ai/sbert_large_nlu_ru model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/sberbank-ai_sbert_large_nlu_ru. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Успокойтесь. Я выросла среди евреев. Так что не надо мне рассказывать. Есть разные евреи. Я не спорю\n",
      "Processing messages using DeepPavlov/rubert-base-cased-sentence model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/DeepPavlov_rubert-base-cased-sentence. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-multilingual-mpnet-base-v2 model\n",
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-xlm-r-multilingual-v1 model\n",
      "Processing complete\n",
      "Processing messages using sberbank-ai/sbert_large_nlu_ru model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/sberbank-ai_sbert_large_nlu_ru. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "У вас всех слишком много стереотипов.Например, что английский в англоязычных странах изучают только \n",
      "Processing messages using DeepPavlov/rubert-base-cased-sentence model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/DeepPavlov_rubert-base-cased-sentence. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-multilingual-mpnet-base-v2 model\n",
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-xlm-r-multilingual-v1 model\n",
      "Processing complete\n",
      "Processing messages using sberbank-ai/sbert_large_nlu_ru model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/sberbank-ai_sbert_large_nlu_ru. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Ой, ну что же вы такие слабые пошли, отличницы. Идите и даже не думайте. Конечно, интересно. Я обожа\n",
      "Processing messages using DeepPavlov/rubert-base-cased-sentence model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/DeepPavlov_rubert-base-cased-sentence. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-multilingual-mpnet-base-v2 model\n",
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-xlm-r-multilingual-v1 model\n",
      "Processing complete\n",
      "Processing messages using sberbank-ai/sbert_large_nlu_ru model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/sberbank-ai_sbert_large_nlu_ru. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Автор может переехать за границу. Оттуда оплачивать сиделку.Это было решающим моментом, почему я не \n",
      "Processing messages using DeepPavlov/rubert-base-cased-sentence model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/DeepPavlov_rubert-base-cased-sentence. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-multilingual-mpnet-base-v2 model\n",
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-xlm-r-multilingual-v1 model\n",
      "Processing complete\n",
      "Processing messages using sberbank-ai/sbert_large_nlu_ru model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/sberbank-ai_sbert_large_nlu_ru. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Да ладно вам, наивным, про 8 лет. Я в 8 лет решила стать кандидатом наук. И ни на день не забывала о\n",
      "Processing messages using DeepPavlov/rubert-base-cased-sentence model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/DeepPavlov_rubert-base-cased-sentence. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-multilingual-mpnet-base-v2 model\n",
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-xlm-r-multilingual-v1 model\n",
      "Processing complete\n",
      "Processing messages using sberbank-ai/sbert_large_nlu_ru model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/sberbank-ai_sbert_large_nlu_ru. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Чего такой спор? Учеба в Канаде РАЗНАЯ. Одно дело - на инженера в универе, а другое - в колледже на \n",
      "Processing messages using DeepPavlov/rubert-base-cased-sentence model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/DeepPavlov_rubert-base-cased-sentence. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-multilingual-mpnet-base-v2 model\n",
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-xlm-r-multilingual-v1 model\n",
      "Processing complete\n",
      "Processing messages using sberbank-ai/sbert_large_nlu_ru model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/sberbank-ai_sbert_large_nlu_ru. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Здесь просто необходимо все скрывать. Все играют роли. А ненависть на рабочем месте хлещет аж будь з\n",
      "Processing messages using DeepPavlov/rubert-base-cased-sentence model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/DeepPavlov_rubert-base-cased-sentence. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-multilingual-mpnet-base-v2 model\n",
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-xlm-r-multilingual-v1 model\n",
      "Processing complete\n",
      "Processing messages using sberbank-ai/sbert_large_nlu_ru model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/sberbank-ai_sbert_large_nlu_ru. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Это у вас в Америке можно устроиться историком по 20 долларов в час. А в Канаде и мест нет. У нас ис\n",
      "Processing messages using DeepPavlov/rubert-base-cased-sentence model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/DeepPavlov_rubert-base-cased-sentence. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-multilingual-mpnet-base-v2 model\n",
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-xlm-r-multilingual-v1 model\n",
      "Processing complete\n",
      "Processing messages using sberbank-ai/sbert_large_nlu_ru model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/sberbank-ai_sbert_large_nlu_ru. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "А я в 8 лет приняла решение любой ценой получить диплом кандидата наук и место на кафедре. И каждую \n",
      "Processing messages using DeepPavlov/rubert-base-cased-sentence model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/DeepPavlov_rubert-base-cased-sentence. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-multilingual-mpnet-base-v2 model\n",
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-xlm-r-multilingual-v1 model\n",
      "Processing complete\n",
      "Processing messages using sberbank-ai/sbert_large_nlu_ru model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/sberbank-ai_sbert_large_nlu_ru. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "А в чем заблуждение? Мне на работе присылали такие имейлы. Мои знакомые убивают белок. Полно таких. \n",
      "Processing messages using DeepPavlov/rubert-base-cased-sentence model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/DeepPavlov_rubert-base-cased-sentence. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-multilingual-mpnet-base-v2 model\n",
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-xlm-r-multilingual-v1 model\n",
      "Processing complete\n",
      "Processing messages using sberbank-ai/sbert_large_nlu_ru model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/sberbank-ai_sbert_large_nlu_ru. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "А Вы работаете в отделе кадров и проверили каждый диплом, который Филолог получила после диплома фил\n",
      "Processing messages using DeepPavlov/rubert-base-cased-sentence model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/DeepPavlov_rubert-base-cased-sentence. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-multilingual-mpnet-base-v2 model\n",
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-xlm-r-multilingual-v1 model\n",
      "Processing complete\n",
      "Processing messages using sberbank-ai/sbert_large_nlu_ru model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/sberbank-ai_sbert_large_nlu_ru. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "У нас очень много крутых спецов. В кого ни плюнь - по 2 ученые степени о том, какие они придумали те\n",
      "Processing messages using DeepPavlov/rubert-base-cased-sentence model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/DeepPavlov_rubert-base-cased-sentence. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-multilingual-mpnet-base-v2 model\n",
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-xlm-r-multilingual-v1 model\n",
      "Processing complete\n",
      "Processing messages using sberbank-ai/sbert_large_nlu_ru model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/sberbank-ai_sbert_large_nlu_ru. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Мы снижаем оценку за глагол ютилайз и требует глагол use, вместо finalize требуем finish. И так дале\n",
      "Processing messages using DeepPavlov/rubert-base-cased-sentence model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/DeepPavlov_rubert-base-cased-sentence. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-multilingual-mpnet-base-v2 model\n",
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-xlm-r-multilingual-v1 model\n",
      "Processing complete\n",
      "Processing messages using sberbank-ai/sbert_large_nlu_ru model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/sberbank-ai_sbert_large_nlu_ru. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Все иностранцы, которые изучают русский язык, всегда поражаются, как русские плохо относятся друг к \n",
      "Processing messages using DeepPavlov/rubert-base-cased-sentence model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/DeepPavlov_rubert-base-cased-sentence. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-multilingual-mpnet-base-v2 model\n",
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-xlm-r-multilingual-v1 model\n",
      "Processing complete\n",
      "Processing messages using sberbank-ai/sbert_large_nlu_ru model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/sberbank-ai_sbert_large_nlu_ru. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Если бы я на такое обращала внимание, я бы не дожила до 5 лет. А зачем убирать за кем-то? Чужое трог\n",
      "Processing messages using DeepPavlov/rubert-base-cased-sentence model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/DeepPavlov_rubert-base-cased-sentence. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-multilingual-mpnet-base-v2 model\n",
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-xlm-r-multilingual-v1 model\n",
      "Processing complete\n",
      "Processing messages using sberbank-ai/sbert_large_nlu_ru model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/sberbank-ai_sbert_large_nlu_ru. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Угу, вторую диссертацию пишет - точно малообразованный кадр - дефективный идиот. В богатых странах д\n",
      "Processing messages using DeepPavlov/rubert-base-cased-sentence model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/DeepPavlov_rubert-base-cased-sentence. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-multilingual-mpnet-base-v2 model\n",
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-xlm-r-multilingual-v1 model\n",
      "Processing complete\n",
      "Processing messages using sberbank-ai/sbert_large_nlu_ru model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/sberbank-ai_sbert_large_nlu_ru. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Я живу в обществе, где вообще никто никому не нужен. Тем не менее, все лингвисты где-то работают с у\n",
      "Processing messages using DeepPavlov/rubert-base-cased-sentence model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/DeepPavlov_rubert-base-cased-sentence. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-multilingual-mpnet-base-v2 model\n",
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-xlm-r-multilingual-v1 model\n",
      "Processing complete\n",
      "Processing messages using sberbank-ai/sbert_large_nlu_ru model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/sberbank-ai_sbert_large_nlu_ru. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Вы в другой категории. Вы потомок шведов, которые пострадали при Советской власти. Вы вообще вернули\n",
      "Processing messages using DeepPavlov/rubert-base-cased-sentence model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/DeepPavlov_rubert-base-cased-sentence. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-multilingual-mpnet-base-v2 model\n",
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-xlm-r-multilingual-v1 model\n",
      "Processing complete\n",
      "Processing messages using sberbank-ai/sbert_large_nlu_ru model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/sberbank-ai_sbert_large_nlu_ru. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Да, это серьезный удар по мозгам. Диалоги. Краткие диалоги. Они основаны на структурах.Кстати, есть \n",
      "Processing messages using DeepPavlov/rubert-base-cased-sentence model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/DeepPavlov_rubert-base-cased-sentence. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-multilingual-mpnet-base-v2 model\n",
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-xlm-r-multilingual-v1 model\n",
      "Processing complete\n",
      "Processing messages using sberbank-ai/sbert_large_nlu_ru model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/sberbank-ai_sbert_large_nlu_ru. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Итак, пейзаж. Представим себе курс, имеющий отношение к статистике, который преподают аспирантам одн\n",
      "Processing messages using DeepPavlov/rubert-base-cased-sentence model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/DeepPavlov_rubert-base-cased-sentence. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-multilingual-mpnet-base-v2 model\n",
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-xlm-r-multilingual-v1 model\n",
      "Processing complete\n",
      "Processing messages using sberbank-ai/sbert_large_nlu_ru model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/sberbank-ai_sbert_large_nlu_ru. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Вот это я зажгла... Столько откликов... видимо, животрепещущая тема.Я скажу кратко. Я не репетитор п\n",
      "Processing messages using DeepPavlov/rubert-base-cased-sentence model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/DeepPavlov_rubert-base-cased-sentence. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-multilingual-mpnet-base-v2 model\n",
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-xlm-r-multilingual-v1 model\n",
      "Processing complete\n",
      "Processing messages using sberbank-ai/sbert_large_nlu_ru model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/sberbank-ai_sbert_large_nlu_ru. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Никакие блоги я бесплатно не веду. За деньги когда-то вела. И за визы. То, о чем я пишу, - одна стор\n",
      "Processing messages using DeepPavlov/rubert-base-cased-sentence model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/DeepPavlov_rubert-base-cased-sentence. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-multilingual-mpnet-base-v2 model\n",
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-xlm-r-multilingual-v1 model\n",
      "Processing complete\n",
      "Processing messages using sberbank-ai/sbert_large_nlu_ru model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/sberbank-ai_sbert_large_nlu_ru. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Как мне нравится картинка. Отличная картинка.Да нет, я такая трезвая. Вот сейчас пойду праздновать П\n",
      "Processing messages using DeepPavlov/rubert-base-cased-sentence model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/DeepPavlov_rubert-base-cased-sentence. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-multilingual-mpnet-base-v2 model\n",
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-xlm-r-multilingual-v1 model\n",
      "Processing complete\n",
      "Processing messages using sberbank-ai/sbert_large_nlu_ru model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/sberbank-ai_sbert_large_nlu_ru. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Как потомственная отличница в энном поколении скажу, что у отличников, в целом, все благополучно. Но\n",
      "Processing messages using DeepPavlov/rubert-base-cased-sentence model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/DeepPavlov_rubert-base-cased-sentence. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-multilingual-mpnet-base-v2 model\n",
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-xlm-r-multilingual-v1 model\n",
      "Processing complete\n",
      "Processing messages using sberbank-ai/sbert_large_nlu_ru model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/sberbank-ai_sbert_large_nlu_ru. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Делаем ставки! Не факт, что он получит то, что хочет.Видели мы таких людей. Многих поимели, как хоте\n",
      "Processing messages using DeepPavlov/rubert-base-cased-sentence model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/DeepPavlov_rubert-base-cased-sentence. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-multilingual-mpnet-base-v2 model\n",
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-xlm-r-multilingual-v1 model\n",
      "Processing complete\n",
      "Processing messages using sberbank-ai/sbert_large_nlu_ru model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/sberbank-ai_sbert_large_nlu_ru. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Ха ха! Вот насмешили меня в соседней теме. Говорят, что удивляются, как это иностранцы с акцентом пр\n",
      "Processing messages using DeepPavlov/rubert-base-cased-sentence model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/DeepPavlov_rubert-base-cased-sentence. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-multilingual-mpnet-base-v2 model\n",
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-xlm-r-multilingual-v1 model\n",
      "Processing complete\n",
      "Processing messages using sberbank-ai/sbert_large_nlu_ru model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/sberbank-ai_sbert_large_nlu_ru. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Повеселили. Тут можно измерять бесконечно. Год рождения филолога, год поступления, год получения дип\n",
      "Processing messages using DeepPavlov/rubert-base-cased-sentence model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/DeepPavlov_rubert-base-cased-sentence. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-multilingual-mpnet-base-v2 model\n",
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-xlm-r-multilingual-v1 model\n",
      "Processing complete\n",
      "Processing messages using sberbank-ai/sbert_large_nlu_ru model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/sberbank-ai_sbert_large_nlu_ru. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "А вы готовьтесь. Приготовьте наручники и дубинки. И сразу всех зеков по струночке. Их уже в тюряге к\n",
      "Processing messages using DeepPavlov/rubert-base-cased-sentence model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/DeepPavlov_rubert-base-cased-sentence. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-multilingual-mpnet-base-v2 model\n",
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-xlm-r-multilingual-v1 model\n",
      "Processing complete\n",
      "Processing messages using sberbank-ai/sbert_large_nlu_ru model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/sberbank-ai_sbert_large_nlu_ru. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Вы МНЕ будете рассказывать, к чему я имею отношение???К Вашему сведению, курсы коммуникации сейчас п\n",
      "Processing messages using DeepPavlov/rubert-base-cased-sentence model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/DeepPavlov_rubert-base-cased-sentence. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-multilingual-mpnet-base-v2 model\n",
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-xlm-r-multilingual-v1 model\n",
      "Processing complete\n",
      "Processing messages using sberbank-ai/sbert_large_nlu_ru model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/sberbank-ai_sbert_large_nlu_ru. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Нужно взять персоналии тех, кто критикует, поднять их собственные дипломы, найти там тройки на госэк\n",
      "Processing messages using DeepPavlov/rubert-base-cased-sentence model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/DeepPavlov_rubert-base-cased-sentence. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-multilingual-mpnet-base-v2 model\n",
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-xlm-r-multilingual-v1 model\n",
      "Processing complete\n",
      "Processing messages using sberbank-ai/sbert_large_nlu_ru model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/sberbank-ai_sbert_large_nlu_ru. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Это про текст на английском языке. На высоких уровнях райтинга требуется активное употребление соеди\n",
      "Processing messages using DeepPavlov/rubert-base-cased-sentence model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/DeepPavlov_rubert-base-cased-sentence. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-multilingual-mpnet-base-v2 model\n",
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-xlm-r-multilingual-v1 model\n",
      "Processing complete\n",
      "Processing messages using sberbank-ai/sbert_large_nlu_ru model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/sberbank-ai_sbert_large_nlu_ru. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Учителей они заменят легко. Они не на роботов их заменят. На данном этапе в богатых странах они прос\n",
      "Processing messages using DeepPavlov/rubert-base-cased-sentence model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/DeepPavlov_rubert-base-cased-sentence. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-multilingual-mpnet-base-v2 model\n",
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-xlm-r-multilingual-v1 model\n",
      "Processing complete\n",
      "Processing messages using sberbank-ai/sbert_large_nlu_ru model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/sberbank-ai_sbert_large_nlu_ru. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Если вы помните те времена, когда в СССР возле кабинета ЛОР-врача зимой во время эпидемии гриппа сид\n",
      "Processing messages using DeepPavlov/rubert-base-cased-sentence model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/DeepPavlov_rubert-base-cased-sentence. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-multilingual-mpnet-base-v2 model\n",
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-xlm-r-multilingual-v1 model\n",
      "Processing complete\n",
      "Processing messages using sberbank-ai/sbert_large_nlu_ru model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/sberbank-ai_sbert_large_nlu_ru. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Да уж реальнее некуда. Написано, конечно, через пень колоду, но реальнее некуда. Другое дело, что жи\n",
      "Processing messages using DeepPavlov/rubert-base-cased-sentence model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/DeepPavlov_rubert-base-cased-sentence. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-multilingual-mpnet-base-v2 model\n",
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-xlm-r-multilingual-v1 model\n",
      "Processing complete\n",
      "Processing messages using sberbank-ai/sbert_large_nlu_ru model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/sberbank-ai_sbert_large_nlu_ru. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Ха! Это Вы еще не видели моих коллег, если я Вам не понравилась. Вы что, на моих лекциях бываете? Вы\n",
      "Processing messages using DeepPavlov/rubert-base-cased-sentence model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/DeepPavlov_rubert-base-cased-sentence. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-multilingual-mpnet-base-v2 model\n",
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-xlm-r-multilingual-v1 model\n",
      "Processing complete\n",
      "Processing messages using sberbank-ai/sbert_large_nlu_ru model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/sberbank-ai_sbert_large_nlu_ru. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "СтереотипыВы слишком погрузились в стереотипы. Вы думаете, что райтер - это минимум Василий Песков о\n",
      "Processing messages using DeepPavlov/rubert-base-cased-sentence model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/DeepPavlov_rubert-base-cased-sentence. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-multilingual-mpnet-base-v2 model\n",
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-xlm-r-multilingual-v1 model\n",
      "Processing complete\n",
      "Processing messages using sberbank-ai/sbert_large_nlu_ru model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/sberbank-ai_sbert_large_nlu_ru. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Меня когда сократили в американской редакции, так утешать водили в ресторан, покупали мне любимые бл\n",
      "Processing messages using DeepPavlov/rubert-base-cased-sentence model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/DeepPavlov_rubert-base-cased-sentence. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-multilingual-mpnet-base-v2 model\n",
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-xlm-r-multilingual-v1 model\n",
      "Processing complete\n",
      "Processing messages using sberbank-ai/sbert_large_nlu_ru model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/sberbank-ai_sbert_large_nlu_ru. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Чтобы писать более утонченным слогом, нужно ВРЕМЯ. Кто-то 5 раз перепишет, кто-то 10 раз перепишет. \n",
      "Processing messages using DeepPavlov/rubert-base-cased-sentence model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/DeepPavlov_rubert-base-cased-sentence. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-multilingual-mpnet-base-v2 model\n",
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-xlm-r-multilingual-v1 model\n",
      "Processing complete\n",
      "Processing messages using sberbank-ai/sbert_large_nlu_ru model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/sberbank-ai_sbert_large_nlu_ru. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Смотря что и как под этим заголовком преподавать. В Канаде преподают сложнейшую грамматику на специа\n",
      "Processing messages using DeepPavlov/rubert-base-cased-sentence model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/DeepPavlov_rubert-base-cased-sentence. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-multilingual-mpnet-base-v2 model\n",
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-xlm-r-multilingual-v1 model\n",
      "Processing complete\n",
      "Processing messages using sberbank-ai/sbert_large_nlu_ru model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/sberbank-ai_sbert_large_nlu_ru. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      ">>>>>>>>>Вы серьезно думаете, что общеобразовательные предметы, преподаваемые и вузе, способны что-т\n",
      "Processing messages using DeepPavlov/rubert-base-cased-sentence model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/DeepPavlov_rubert-base-cased-sentence. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-multilingual-mpnet-base-v2 model\n",
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-xlm-r-multilingual-v1 model\n",
      "Processing complete\n",
      "Processing messages using sberbank-ai/sbert_large_nlu_ru model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/sberbank-ai_sbert_large_nlu_ru. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Так это же сравнение.ОК. На первый взгляд кажется, что в ПТУ не нужно сложная грамматика. Я не буду \n",
      "Processing messages using DeepPavlov/rubert-base-cased-sentence model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/DeepPavlov_rubert-base-cased-sentence. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-multilingual-mpnet-base-v2 model\n",
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-xlm-r-multilingual-v1 model\n",
      "Processing complete\n",
      "Processing messages using sberbank-ai/sbert_large_nlu_ru model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/sberbank-ai_sbert_large_nlu_ru. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "С другой стороны, родители сдали своих детей в киевский политех, чтобы вытолкать их за границу прогр\n",
      "Processing messages using DeepPavlov/rubert-base-cased-sentence model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/DeepPavlov_rubert-base-cased-sentence. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-multilingual-mpnet-base-v2 model\n",
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-xlm-r-multilingual-v1 model\n",
      "Processing complete\n",
      "Processing messages using sberbank-ai/sbert_large_nlu_ru model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/sberbank-ai_sbert_large_nlu_ru. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Вы просто говорите так, как нужно говорить. В соответствии со словарями ударений для сотрудников рад\n",
      "Processing messages using DeepPavlov/rubert-base-cased-sentence model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/DeepPavlov_rubert-base-cased-sentence. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-multilingual-mpnet-base-v2 model\n",
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-xlm-r-multilingual-v1 model\n",
      "Processing complete\n",
      "Processing messages using sberbank-ai/sbert_large_nlu_ru model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/sberbank-ai_sbert_large_nlu_ru. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Очень много вредительства в том, что детям внушают, что учеба должна им нравиться. Потом будут толпы\n",
      "Processing messages using DeepPavlov/rubert-base-cased-sentence model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/DeepPavlov_rubert-base-cased-sentence. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-multilingual-mpnet-base-v2 model\n",
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-xlm-r-multilingual-v1 model\n",
      "Processing complete\n",
      "Processing messages using sberbank-ai/sbert_large_nlu_ru model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/sberbank-ai_sbert_large_nlu_ru. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "А КТО должен оплатить расходы на все ваши варианты выбора??? КТО? Не бывает и равенства для всех, и \n",
      "Processing messages using DeepPavlov/rubert-base-cased-sentence model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/DeepPavlov_rubert-base-cased-sentence. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-multilingual-mpnet-base-v2 model\n",
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-xlm-r-multilingual-v1 model\n",
      "Processing complete\n",
      "Processing messages using sberbank-ai/sbert_large_nlu_ru model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/sberbank-ai_sbert_large_nlu_ru. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Все понятно, почему такая программа. В России есть нефтянка, есть металлургические заводы, есть обор\n",
      "Processing messages using DeepPavlov/rubert-base-cased-sentence model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/DeepPavlov_rubert-base-cased-sentence. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-multilingual-mpnet-base-v2 model\n",
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-xlm-r-multilingual-v1 model\n",
      "Processing complete\n",
      "Processing messages using sberbank-ai/sbert_large_nlu_ru model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/sberbank-ai_sbert_large_nlu_ru. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "В каждой стране свои возможности в плане образования. Нужно брать то, что можно взять. Вы можете пое\n",
      "Processing messages using DeepPavlov/rubert-base-cased-sentence model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/DeepPavlov_rubert-base-cased-sentence. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-multilingual-mpnet-base-v2 model\n",
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-xlm-r-multilingual-v1 model\n",
      "Processing complete\n",
      "Processing messages using sberbank-ai/sbert_large_nlu_ru model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/sberbank-ai_sbert_large_nlu_ru. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Автор хочет слышать. Так не бывает. Вы услышите маст В бин дан. И напишите ОВ, а это не ОВ, а ХЭВ. И\n",
      "Processing messages using DeepPavlov/rubert-base-cased-sentence model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/DeepPavlov_rubert-base-cased-sentence. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-multilingual-mpnet-base-v2 model\n",
      "Processing complete\n",
      "Processing messages using sentence-transformers/paraphrase-xlm-r-multilingual-v1 model\n",
      "Processing complete\n",
      "Processing messages using sberbank-ai/sbert_large_nlu_ru model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name /root/.cache/torch/sentence_transformers/sberbank-ai_sbert_large_nlu_ru. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete\n",
      "2021-10-14 06:41:00.451048\n"
     ]
    }
   ],
   "source": [
    "df_ts = similarity_search(df_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "73AQd_Jy9LSX"
   },
   "outputs": [],
   "source": [
    "df_ts = pd.read_csv(ts_full_filename, error_bad_lines=False, index_col=False, usecols=['Sample_Id','model','score','Message_Id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2514,
     "status": "ok",
     "timestamp": 1634681663598,
     "user": {
      "displayName": "Kateryna Drogaieva",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB1LQYJH97Lqm6RsCcAADFgzEAa0YxMUT7P4mjlA=s64",
      "userId": "10877377912113147805"
     },
     "user_tz": 420
    },
    "id": "FRJVL0jhts5e",
    "outputId": "56dfae7d-7fef-4ea3-efb5-4ea8bfe31326"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "453\n"
     ]
    }
   ],
   "source": [
    "df_samples = pd.DataFrame({'Sample_Id':df_ts['Sample_Id'].unique().tolist()})\n",
    "Message_Ids=get_samples(100)\n",
    "print(len(Message_Ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cNkWYfVzsk8j"
   },
   "outputs": [],
   "source": [
    "df_samples = Messages[Messages['Message_Id'].isin(Message_Ids)][['Message_Id','message']]\n",
    "df_samples.columns=['Sample_Id','Sample']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 669
    },
    "executionInfo": {
     "elapsed": 574,
     "status": "ok",
     "timestamp": 1634681945433,
     "user": {
      "displayName": "Kateryna Drogaieva",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjB1LQYJH97Lqm6RsCcAADFgzEAa0YxMUT7P4mjlA=s64",
      "userId": "10877377912113147805"
     },
     "user_tz": 420
    },
    "id": "8GR3LtH7tDCi",
    "outputId": "03bc410a-61c7-4bbe-efda-1cc8c2e39bf3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample_Id</th>\n",
       "      <th>Sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1641</th>\n",
       "      <td>102651614</td>\n",
       "      <td>Было похожее приключение. Канадская фирма вста...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1649</th>\n",
       "      <td>102704980</td>\n",
       "      <td>Есть такие.Вариант 1 - у ее мужа бизнес, т.е. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>914</th>\n",
       "      <td>102759461</td>\n",
       "      <td>Ой, да разберется эта шведка без наших советов...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>919</th>\n",
       "      <td>102785964</td>\n",
       "      <td>Потрясают такие эксперты. В США едут, когда не...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1654</th>\n",
       "      <td>102795821</td>\n",
       "      <td>Тут много спорили про туалеты. В том числе, пр...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1653</th>\n",
       "      <td>102803426</td>\n",
       "      <td>Мы не знаем отношений в этой семье. Ее родител...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1659</th>\n",
       "      <td>102826001</td>\n",
       "      <td>Никто никуда не просится - у них кормушки там,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1664</th>\n",
       "      <td>102844400</td>\n",
       "      <td>Не скажите. Ему показали секреты бизнеса. А по...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>102904259</td>\n",
       "      <td>В любой бюрократической работе учат на месте. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>102910086</td>\n",
       "      <td>Нюансов может быть 110, а может быть 2110 или ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>102937305</td>\n",
       "      <td>Есть миллион сфер, где профессиональные качест...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1670</th>\n",
       "      <td>102938786</td>\n",
       "      <td>Я даже вам отвечу. Папулька и мамулька в принц...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>102959247</td>\n",
       "      <td>Как это не филолог? Ваковский доцент по самой ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>102959827</td>\n",
       "      <td>То, что она нудит и жалуется, не означает ниче...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>102984851</td>\n",
       "      <td>За хорошие вложения и унижения можно и 5 образ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>102994955</td>\n",
       "      <td>Никаких старушек они не трогают за руки. Я пер...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1683</th>\n",
       "      <td>103095201</td>\n",
       "      <td>Да ладно, в Канаде на мусоровозах ездят либо н...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1685</th>\n",
       "      <td>103098100</td>\n",
       "      <td>В Украине все живут по-разному. Там все бизнес...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>943</th>\n",
       "      <td>103098408</td>\n",
       "      <td>\"Полноценный\" сотрудник принесет им те проблем...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1684</th>\n",
       "      <td>103098873</td>\n",
       "      <td>В смысле? Абсолютно то же самое можно расписат...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sample_Id                                             Sample\n",
       "1641  102651614  Было похожее приключение. Канадская фирма вста...\n",
       "1649  102704980  Есть такие.Вариант 1 - у ее мужа бизнес, т.е. ...\n",
       "914   102759461  Ой, да разберется эта шведка без наших советов...\n",
       "919   102785964  Потрясают такие эксперты. В США едут, когда не...\n",
       "1654  102795821  Тут много спорили про туалеты. В том числе, пр...\n",
       "1653  102803426  Мы не знаем отношений в этой семье. Ее родител...\n",
       "1659  102826001  Никто никуда не просится - у них кормушки там,...\n",
       "1664  102844400  Не скажите. Ему показали секреты бизнеса. А по...\n",
       "932   102904259  В любой бюрократической работе учат на месте. ...\n",
       "933   102910086  Нюансов может быть 110, а может быть 2110 или ...\n",
       "934   102937305  Есть миллион сфер, где профессиональные качест...\n",
       "1670  102938786  Я даже вам отвечу. Папулька и мамулька в принц...\n",
       "937   102959247  Как это не филолог? Ваковский доцент по самой ...\n",
       "938   102959827  То, что она нудит и жалуется, не означает ниче...\n",
       "939   102984851  За хорошие вложения и унижения можно и 5 образ...\n",
       "940   102994955  Никаких старушек они не трогают за руки. Я пер...\n",
       "1683  103095201  Да ладно, в Канаде на мусоровозах ездят либо н...\n",
       "1685  103098100  В Украине все живут по-разному. Там все бизнес...\n",
       "943   103098408  \"Полноценный\" сотрудник принесет им те проблем...\n",
       "1684  103098873  В смысле? Абсолютно то же самое можно расписат..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_samples.sort_values(['Sample_Id'], ascending=True).tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZEw8kp-CIZLP"
   },
   "outputs": [],
   "source": [
    "df_samples.to_csv(ps_full_filename, header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPbOeDtthoMuCJgFiSGFgwf",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Eva Text Similarity.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
